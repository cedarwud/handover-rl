# æ¨¡å—åŒ–RLæ¶æ„é‡æ„è®¾è®¡

**ç›®æ ‡**: ä» DQN-only æ¶æ„ â†’ æ”¯æŒå¤šç§RLç®—æ³•çš„é€šç”¨æ¡†æ¶

**åˆ›å»ºæ—¶é—´**: 2025-10-19
**çŠ¶æ€**: ğŸš§ è®¾è®¡é˜¶æ®µ

---

## âš ï¸ IMPORTANT: PROJECT SCOPE CLARIFICATION

**æœ¬é¡¹ç›®å®æ–½èŒƒå›´** (Phase 1-2 only, 2-3 weeks):
- âœ… **Phase 1**: DQN refactoring to modular architecture
- âœ… **Phase 2**: Verify architecture supports multiple algorithms (proof of concept)
- âœ… **Baseline Framework**: 1 RL baseline (DQN) + 3 Rule-based baselines

**æ˜ç¡®è¯´æ˜**:
- âŒ **Phase 3-4 (D3QN, PPO, A2C) ä¸åœ¨å½“å‰é¡¹ç›®å®æ–½èŒƒå›´å†…**
- âŒ **ä¸éœ€è¦å®ç°å…¶ä»– RL ç®—æ³•**ï¼ˆD3QNã€PPOã€A2Cã€Rainbowã€SAC ç­‰ï¼‰
- â­ **æœªæ¥å·¥ä½œ**: ç”¨æˆ·è‡ªå·±çš„ç®—æ³• vs ä»¥ä¸Š 4 ä¸ª baselines

**æœ¬æ–‡æ¡£ç”¨é€”**:
- âœ… Architecture design reference (å¦‚ä½•è®¾è®¡å¯æ‰©å±•æ¶æ„)
- âœ… Phase 1-2 implementation guide (DQN é‡æ„ + æ¶æ„éªŒè¯)
- âŒ NOT a commitment to implement all listed algorithms

è¯¦è§ [BASELINE_ALGORITHMS.md](../algorithms/BASELINE_ALGORITHMS.md) å®Œæ•´è¯´æ˜

---

## âš ï¸ æ ¸å¿ƒåŠŸèƒ½ä¿ç•™æ¸…å•ï¼ˆP0 - CRITICALï¼‰

**é‡è¦**: ä»¥ä¸‹åŠŸèƒ½æ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼ˆREADME.md "Novel Aspects"ï¼‰ï¼Œé‡æ„æ—¶**å¿…é¡»å®Œæ•´ä¿ç•™**ã€‚

### 1. Multi-Level Training Strategy â­â­â­â­â­

**é‡è¦æ€§**: README.md åˆ—ä¸º "Novel Aspects #1" - ç ”ç©¶è´¡çŒ®çš„ç¬¬ä¸€ä¸ªåˆ›æ–°ç‚¹

**ç°æœ‰åŠŸèƒ½**:
- 6 ä¸ªè®­ç»ƒçº§åˆ«ï¼ˆLevel 0-5ï¼‰
- æ¸è¿›å¼éªŒè¯ç­–ç•¥ï¼š10åˆ†é’Ÿ â†’ 35å°æ—¶
- å¿«é€Ÿè¿­ä»£å¼€å‘ï¼ˆLevel 1: 2å°æ—¶ vs Level 5: 35å°æ—¶ï¼‰

**å¿…é¡»ä¿ç•™**:
```bash
# é‡æ„åå¿…é¡»æ”¯æŒ
train.py --algorithm dqn --level 0  # 10åˆ†é’Ÿ - Smoke test
train.py --algorithm dqn --level 1  # 2å°æ—¶ - Quick validation
train.py --algorithm dqn --level 3  # 10å°æ—¶ - Validation (æ¨è)
train.py --algorithm dqn --level 5  # 35å°æ—¶ - Full training
```

**å®ç°è¦æ±‚**:
- [ ] åˆ›å»º `src/configs/training_levels.py` æˆ– `config/levels/*.yaml`
- [ ] train.py å¿…é¡»æ”¯æŒ `--level {0,1,2,3,4,5}` å‚æ•°
- [ ] æ‰€æœ‰ 6 ä¸ª level çš„é…ç½®å¿…é¡»å‡†ç¡®ï¼ˆå«æ˜Ÿæ•°ã€episodesã€æ—¶é—´ï¼‰
- [ ] æ‰€æœ‰ç®—æ³•ï¼ˆDQN, PPO, A2Cï¼‰éƒ½å¿…é¡»æ”¯æŒæ‰€æœ‰ level

**å­¦æœ¯ä»·å€¼**:
- è¿™æ˜¯è®ºæ–‡çš„ä¸€ä¸ªç‹¬ç«‹è´¡çŒ®ç‚¹
- å±•ç¤ºå¦‚ä½•é«˜æ•ˆè¿›è¡Œ RL å®éªŒ
- å…¶ä»–ç ”ç©¶è€…å¯ä»¥å¤åˆ¶æ­¤æ–¹æ³•

**éªŒè¯æ ‡å‡†**:
- âœ… Level 1 è®­ç»ƒæ—¶é—´ â‰ˆ 2å°æ—¶
- âœ… Level 3 è®­ç»ƒæ—¶é—´ â‰ˆ 10å°æ—¶
- âœ… Level 5 è®­ç»ƒæ—¶é—´ â‰ˆ 35å°æ—¶
- âœ… æ‰€æœ‰ç®—æ³•åœ¨æ‰€æœ‰ level éƒ½å¯è¿è¡Œ

---

### 2. Gymnasium Environment âœ…

**ç°æœ‰åŠŸèƒ½**: SatelliteHandoverEnv (å·²ç®—æ³•æ— å…³)

**ä¿ç•™çŠ¶æ€**: âœ… å·²å®Œæˆï¼Œæ— éœ€ä¿®æ”¹

---

### 3. Real TLE Data + Complete Physics âœ…

**ç°æœ‰åŠŸèƒ½**:
- Space-Track.org TLE æ•°æ®
- ITU-R + 3GPP å®Œæ•´ç‰©ç†æ¨¡å‹
- æ— ç®€åŒ–ç®—æ³•ã€æ—  mock æ•°æ®

**ä¿ç•™çŠ¶æ€**: âœ… å·²å®Œæˆï¼Œé‡æ„ä¸å½±å“

---

## ğŸ¯ é‡æ„ç›®æ ‡

### ä»å•ä¸€ç®—æ³•åˆ°å¤šç®—æ³•æ¡†æ¶

**å½“å‰æ¶æ„** (DQN-only):
```python
train_online_rl.py
â”œâ”€â”€ ç¡¬ç¼–ç  DQNAgentV2
â”œâ”€â”€ ç¡¬ç¼–ç  per-step update (off-policy)
â”œâ”€â”€ ç¡¬ç¼–ç  epsilon-greedy exploration
â””â”€â”€ ç¡¬ç¼–ç  experience replay buffer
```

**ç›®æ ‡æ¶æ„** (Algorithm-Agnostic):
```python
train.py --algorithm {dqn|ppo|a2c|sac}
â”œâ”€â”€ Trainer Layer (Off-policy / On-policy)
â”œâ”€â”€ Agent Layer (DQN / PPO / A2C / SAC)
â””â”€â”€ Environment Layer (Gymnasium âœ… Already done)
```

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ä¸‰å±‚è§£è€¦æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: Training Entry Point          â”‚
â”‚  train.py (ç»Ÿä¸€å…¥å£)                     â”‚
â”‚  â”œâ”€â”€ --algorithm {dqn, ppo, a2c}        â”‚
â”‚  â”œâ”€â”€ --level {0,1,2,3,4,5}              â”‚
â”‚  â””â”€â”€ --config algorithms/xxx_config.yamlâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: Trainer (è®­ç»ƒé€»è¾‘)            â”‚
â”‚  â”œâ”€â”€ OffPolicyTrainer (DQN, SAC, ...)  â”‚
â”‚  â”‚   - Experience replay buffer         â”‚
â”‚  â”‚   - Per-step update                  â”‚
â”‚  â”‚   - Target network sync              â”‚
â”‚  â””â”€â”€ OnPolicyTrainer (PPO, A2C, ...)   â”‚
â”‚      - Trajectory collection            â”‚
â”‚      - Batch update after episode       â”‚
â”‚      - GAE advantage calculation        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: Agent (ç®—æ³•å®ç°)              â”‚
â”‚  â”œâ”€â”€ BaseAgent (ç»Ÿä¸€æ¥å£)               â”‚
â”‚  â”œâ”€â”€ DQNAgent                            â”‚
â”‚  â”œâ”€â”€ DoubleDQNAgent                      â”‚
â”‚  â”œâ”€â”€ DuelingDQNAgent                     â”‚
â”‚  â”œâ”€â”€ D3QNAgent                           â”‚
â”‚  â”œâ”€â”€ PPOAgent                            â”‚
â”‚  â”œâ”€â”€ A2CAgent                            â”‚
â”‚  â””â”€â”€ SACAgent (future)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 4: Environment                   â”‚
â”‚  SatelliteHandoverEnv (Gymnasium)       â”‚
â”‚  âœ… Already algorithm-agnostic          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ æ ¸å¿ƒç»„ä»¶è®¾è®¡

### 1. BaseAgent æ¥å£ (src/agents/base_agent.py)

```python
from abc import ABC, abstractmethod
from typing import Any, Dict, Tuple
import numpy as np

class BaseAgent(ABC):
    """
    æ‰€æœ‰RL Agentçš„ç»Ÿä¸€æ¥å£

    è®¾è®¡åŸåˆ™:
    - ç»Ÿä¸€æ–¹æ³•å (select_action, update, save, load)
    - çµæ´»å‚æ•°ç­¾å (ä¸åŒç®—æ³•çš„updateå¯ä»¥æœ‰ä¸åŒå‚æ•°)
    - æ ‡å‡†è¿”å›æ ¼å¼
    """

    @abstractmethod
    def select_action(self, state: np.ndarray, deterministic: bool = False) -> int:
        """
        é€‰æ‹©åŠ¨ä½œ

        Args:
            state: è§‚æµ‹çŠ¶æ€ (shape: [12] for satellite handover)
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
                - True: è¯„ä¼°æ¨¡å¼ï¼Œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
                - False: è®­ç»ƒæ¨¡å¼ï¼Œæ¢ç´¢å¼é€‰æ‹©

        Returns:
            action: åŠ¨ä½œç´¢å¼• (0 or 1 for binary handover)

        NOTE: ä¸åŒç®—æ³•çš„å®ç°ï¼š
        - DQN: epsilon-greedy exploration
        - PPO: sample from policy distribution
        - A2C: sample from policy distribution
        """
        pass

    @abstractmethod
    def update(self, *args, **kwargs) -> float:
        """
        æ›´æ–°Agent

        NOTE: ä¸åŒç®—æ³•çš„updateç­¾åä¸åŒï¼Œè¿™æ˜¯åˆç†çš„ï¼š
        - DQN: update(state, action, reward, next_state, done)
              æˆ– update(batch) for batch update
        - PPO: update(trajectory: List[Dict])
        - A2C: update(trajectory: List[Dict])

        Returns:
            loss: è®­ç»ƒæŸå¤±
        """
        pass

    @abstractmethod
    def save(self, path: str) -> None:
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶"""
        pass

    @abstractmethod
    def load(self, path: str) -> None:
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹"""
        pass

    # å¯é€‰çš„å›è°ƒæ–¹æ³•ï¼ˆå­ç±»å¯ä»¥è¦†ç›–ï¼‰
    def on_episode_start(self) -> None:
        """Episodeå¼€å§‹æ—¶çš„å›è°ƒ"""
        pass

    def on_episode_end(self, episode_reward: float) -> None:
        """Episodeç»“æŸæ—¶çš„å›è°ƒ"""
        pass

    def get_config(self) -> Dict[str, Any]:
        """è¿”å›Agenté…ç½®ï¼ˆç”¨äºä¿å­˜/è®°å½•ï¼‰"""
        return {}
```

### 2. OffPolicyTrainer (src/trainers/off_policy_trainer.py)

```python
class OffPolicyTrainer:
    """
    Off-policyç®—æ³•çš„è®­ç»ƒå™¨ (DQN, SAC, etc.)

    ç‰¹ç‚¹:
    - ä½¿ç”¨ experience replay buffer
    - Per-step update (æ¯æ­¥éƒ½å¯ä»¥æ›´æ–°)
    - å¯ä»¥ä»æ—§ç»éªŒå­¦ä¹ 
    """

    def __init__(self, env, agent, config):
        self.env = env
        self.agent = agent
        self.config = config
        self.replay_buffer = ReplayBuffer(config['buffer_size'])

    def train_episode(self, episode_idx: int) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepisode

        Returns:
            metrics: {'reward': float, 'loss': float, 'handovers': int, ...}
        """
        obs, info = self.env.reset()
        episode_reward = 0
        episode_loss = 0
        num_updates = 0

        while True:
            # 1. Agenté€‰æ‹©åŠ¨ä½œ
            action = self.agent.select_action(obs, deterministic=False)

            # 2. ç¯å¢ƒäº¤äº’
            next_obs, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated

            # 3. å­˜å‚¨åˆ°replay buffer
            self.replay_buffer.add(obs, action, reward, next_obs, done)

            # 4. Per-step update (off-policyç‰¹æ€§)
            if len(self.replay_buffer) >= self.config['min_buffer_size']:
                batch = self.replay_buffer.sample(self.config['batch_size'])
                loss = self.agent.update(batch)
                episode_loss += loss
                num_updates += 1

            episode_reward += reward
            obs = next_obs

            if done:
                break

        avg_loss = episode_loss / num_updates if num_updates > 0 else 0

        return {
            'reward': episode_reward,
            'loss': avg_loss,
            'handovers': info.get('num_handovers', 0),
            'avg_rsrp': info.get('avg_rsrp', 0)
        }
```

### 3. OnPolicyTrainer (src/trainers/on_policy_trainer.py)

```python
class OnPolicyTrainer:
    """
    On-policyç®—æ³•çš„è®­ç»ƒå™¨ (PPO, A2C, etc.)

    ç‰¹ç‚¹:
    - æ”¶é›†å®Œæ•´trajectory
    - Episodeç»“æŸåæ‰æ›´æ–°
    - åªä»å½“å‰ç­–ç•¥æ”¶é›†çš„ç»éªŒå­¦ä¹ 
    """

    def __init__(self, env, agent, config):
        self.env = env
        self.agent = agent
        self.config = config

    def train_episode(self, episode_idx: int) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepisode

        Returns:
            metrics: {'reward': float, 'loss': float, ...}
        """
        obs, info = self.env.reset()
        trajectory = []
        episode_reward = 0

        while True:
            # 1. Agenté€‰æ‹©åŠ¨ä½œï¼ˆåŒæ—¶è¿”å›log_probç”¨äºPPOï¼‰
            action, log_prob = self.agent.select_action(obs, deterministic=False)

            # 2. ç¯å¢ƒäº¤äº’
            next_obs, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated

            # 3. å­˜å‚¨åˆ°trajectory
            trajectory.append({
                'obs': obs,
                'action': action,
                'reward': reward,
                'log_prob': log_prob,  # PPOéœ€è¦
                'done': done
            })

            episode_reward += reward
            obs = next_obs

            if done:
                break

        # 4. Episodeç»“æŸåæ‰æ›´æ–° (on-policyç‰¹æ€§)
        loss = self.agent.update(trajectory)

        return {
            'reward': episode_reward,
            'loss': loss,
            'handovers': info.get('num_handovers', 0),
            'avg_rsrp': info.get('avg_rsrp', 0)
        }
```

### 4. ç»Ÿä¸€è®­ç»ƒå…¥å£ (train.py)

```python
import argparse
from pathlib import Path
import yaml

from src.trainers import OffPolicyTrainer, OnPolicyTrainer
from src.agents import DQNAgent, DoubleDQNAgent, PPOAgent, A2CAgent
from src.environments import SatelliteHandoverEnv
from src.utils import load_stage4_satellites, setup_logging

# ç®—æ³•æ³¨å†Œè¡¨
ALGORITHM_REGISTRY = {
    'dqn': {
        'agent_class': DQNAgent,
        'trainer_class': OffPolicyTrainer,
        'config_file': 'config/algorithms/dqn_config.yaml'
    },
    'double_dqn': {
        'agent_class': DoubleDQNAgent,
        'trainer_class': OffPolicyTrainer,
        'config_file': 'config/algorithms/double_dqn_config.yaml'
    },
    'ppo': {
        'agent_class': PPOAgent,
        'trainer_class': OnPolicyTrainer,
        'config_file': 'config/algorithms/ppo_config.yaml'
    },
    'a2c': {
        'agent_class': A2CAgent,
        'trainer_class': OnPolicyTrainer,
        'config_file': 'config/algorithms/a2c_config.yaml'
    },
}

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--algorithm', type=str,
                       choices=['dqn', 'double_dqn', 'ppo', 'a2c'],
                       required=True,
                       help='RL algorithm to use')
    parser.add_argument('--level', type=int, choices=[0,1,2,3,4,5],
                       default=1,
                       help='Training level (0=smoke test, 5=full training)')
    parser.add_argument('--output-dir', type=str, required=True)
    parser.add_argument('--seed', type=int, default=42)
    args = parser.parse_args()

    # 1. åŠ è½½é…ç½®
    algo_info = ALGORITHM_REGISTRY[args.algorithm]
    with open(algo_info['config_file']) as f:
        algo_config = yaml.safe_load(f)

    with open('config/training_levels.yaml') as f:
        level_config = yaml.safe_load(f)[f'level_{args.level}']

    # 2. åˆ›å»ºEnvironment (ç®—æ³•æ— å…³)
    satellite_ids = load_stage4_satellites(
        constellation_filter='starlink',
        num_satellites=level_config['num_satellites']
    )

    env = SatelliteHandoverEnv(
        satellite_ids=satellite_ids,
        overlap=level_config['overlap']
    )

    # 3. åˆ›å»ºAgent (ç®—æ³•ç‰¹å®š)
    AgentClass = algo_info['agent_class']
    agent = AgentClass(
        obs_space=env.observation_space,
        action_space=env.action_space,
        config=algo_config
    )

    # 4. åˆ›å»ºTrainer (æ ¹æ®ç®—æ³•ç±»å‹)
    TrainerClass = algo_info['trainer_class']
    trainer = TrainerClass(env, agent, algo_config)

    # 5. è®­ç»ƒå¾ªç¯ (ç»Ÿä¸€)
    logger = setup_logging(args.output_dir)

    for episode in range(level_config['num_episodes']):
        metrics = trainer.train_episode(episode)

        if (episode + 1) % level_config['log_interval'] == 0:
            logger.info(f"Episode {episode+1}: reward={metrics['reward']:.2f}, "
                       f"loss={metrics['loss']:.4f}")

        # Checkpoint saving
        if (episode + 1) % level_config['checkpoint_interval'] == 0:
            agent.save(f"{args.output_dir}/checkpoints/ep{episode+1}.pth")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save(f"{args.output_dir}/final_model.pth")
    print(f"âœ… Training completed: {level_config['num_episodes']} episodes")

if __name__ == '__main__':
    main()
```

---

## ğŸ“ ç›®å½•ç»“æ„é‡æ„

### æ–°çš„ä»£ç ç»„ç»‡

```
src/
â”œâ”€â”€ agents/                       # RLç®—æ³•å®ç°
â”‚   â”œâ”€â”€ base_agent.py            # âœ… BaseAgentæ¥å£
â”‚   â”œâ”€â”€ dqn/                     # DQNç³»åˆ—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dqn_agent.py         # âœ… æ ‡å‡†DQN
â”‚   â”‚   â”œâ”€â”€ double_dqn.py        # Double DQN
â”‚   â”‚   â”œâ”€â”€ dueling_dqn.py       # Dueling DQN
â”‚   â”‚   â””â”€â”€ d3qn.py              # Dueling Double DQN
â”‚   â”œâ”€â”€ ppo/                     # PPOç³»åˆ—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ppo_agent.py         # PPO
â”‚   â”‚   â””â”€â”€ networks.py          # Policy/Value networks
â”‚   â”œâ”€â”€ a2c/                     # A2Cç³»åˆ—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ a2c_agent.py         # A2C
â”‚   â””â”€â”€ sac/                     # SAC (future)
â”‚       â””â”€â”€ sac_agent.py
â”œâ”€â”€ trainers/                    # è®­ç»ƒé€»è¾‘ (æ–°å»º)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ off_policy_trainer.py   # Off-policyè®­ç»ƒå™¨
â”‚   â””â”€â”€ on_policy_trainer.py    # On-policyè®­ç»ƒå™¨
â”œâ”€â”€ environments/                # ç¯å¢ƒ (å·²å®Œæˆ)
â”‚   â””â”€â”€ satellite_handover_env.py
â””â”€â”€ utils/                       # å·¥å…·å‡½æ•°
    â”œâ”€â”€ satellite_utils.py
    â””â”€â”€ logging_utils.py

config/
â”œâ”€â”€ training_levels.yaml         # è®­ç»ƒå±‚çº§é…ç½® (ç®—æ³•æ— å…³)
â””â”€â”€ algorithms/                  # ç®—æ³•é…ç½® (æ–°å»º)
    â”œâ”€â”€ dqn_config.yaml
    â”œâ”€â”€ double_dqn_config.yaml
    â”œâ”€â”€ ppo_config.yaml
    â””â”€â”€ a2c_config.yaml

train.py                         # ç»Ÿä¸€è®­ç»ƒå…¥å£ (æ–°å»º)
train_online_rl.py              # æ—§çš„DQN-onlyå…¥å£ (ä¿ç•™ç”¨äºå…¼å®¹)
```

---

## ğŸ”„ é‡æ„ç­–ç•¥

### Phase 1: DQNé‡æ„ (ä¸ç ´åç°æœ‰åŠŸèƒ½)

**ç›®æ ‡**: å°†ç°æœ‰DQNè¿ç§»åˆ°æ–°æ¶æ„ï¼ŒéªŒè¯åŠŸèƒ½ä¸€è‡´æ€§

**æ­¥éª¤**:
1. åˆ›å»º `BaseAgent` æ¥å£
2. åˆ›å»º `OffPolicyTrainer`
3. å°†ç°æœ‰ `DQNAgentV2` é‡æ„ä¸º `DQNAgent` (ç»§æ‰¿BaseAgent)
4. åˆ›å»ºæ–°çš„ `train.py --algorithm dqn`
5. éªŒè¯è®­ç»ƒç»“æœä¸ `train_online_rl.py` ä¸€è‡´

**éªŒè¯æ ‡å‡†**:
- âœ… ç›¸åŒseedä¸‹rewardæ›²çº¿ä¸€è‡´
- âœ… ç›¸åŒè¶…å‚æ•°ä¸‹lossä¸‹é™è¶‹åŠ¿ä¸€è‡´
- âœ… å¯ä»¥åŠ è½½æ—§æ¨¡å‹ç»§ç»­è®­ç»ƒ

**æ—¶é—´**: 1-2å¤©

### Phase 2: PPOå®ç° (éªŒè¯æ¶æ„æ‰©å±•æ€§)

**ç›®æ ‡**: å®ç°PPOï¼ŒéªŒè¯æ–°æ¶æ„çœŸæ­£æ”¯æŒå¤šç®—æ³•

**æ­¥éª¤**:
1. åˆ›å»º `OnPolicyTrainer`
2. å®ç° `PPOAgent`
   - PolicyNetwork (è¾“å‡ºåŠ¨ä½œæ¦‚ç‡)
   - ValueNetwork (ä¼°è®¡çŠ¶æ€ä»·å€¼)
   - GAEè®¡ç®—
   - PPO loss (clipped surrogate objective)
3. åˆ›å»º `config/algorithms/ppo_config.yaml`
4. Level 1å¯¹æ¯”å®éªŒ (DQN vs PPO, 100 episodes)

**éªŒè¯æ ‡å‡†**:
- âœ… PPOè®­ç»ƒæ”¶æ•›ï¼ˆlossä¸‹é™ï¼‰
- âœ… PPO rewardæœ‰æå‡è¶‹åŠ¿
- âœ… å¯ä»¥ç”¨ç›¸åŒçš„ `train.py` å…¥å£

**æ—¶é—´**: 3-5å¤©

### ~~Phase 3: DQNç³»åˆ—æ‰©å±•~~ âŒ NOT IN CURRENT PROJECT SCOPE

**âš ï¸ é‡è¦è¯´æ˜**: Phase 3-4 ä¸åœ¨å½“å‰é¡¹ç›®å®æ–½èŒƒå›´å†…

**é¡¹ç›®èŒƒå›´**: 1 RL baseline (DQN) + 3 Rule-based baselines only
- âŒ ä¸å®ç° Double DQN, Dueling DQN, D3QN
- âŒ ä¸å®ç° PPO, A2C, SAC
- â­ æœªæ¥å·¥ä½œï¼šç”¨æˆ·è‡ªå·±çš„ç®—æ³• vs 4 ä¸ª baselines

**ä¿ç•™åŸå› **: ä½œä¸ºæ¶æ„è®¾è®¡å‚è€ƒï¼Œè¯´æ˜å¦‚ä½•æ‰©å±•æ¡†æ¶ï¼ˆå¦‚æœå°†æ¥éœ€è¦ï¼‰

<details>
<summary>åŸè®¾è®¡ï¼ˆä»…ä¾›å‚è€ƒï¼Œä¸å®æ–½ï¼‰</summary>

**ç›®æ ‡**: æ·»åŠ DQNæ”¹è¿›ç‰ˆæœ¬

**æ­¥éª¤**:
1. Double DQN (1å¤©)
2. Dueling DQN (1å¤©)
3. D3QN (1å¤©)

**æ—¶é—´**: 3å¤©
</details>

### ~~Phase 4: A2Cå®ç° (å¯é€‰)~~ âŒ NOT IN CURRENT PROJECT SCOPE

<details>
<summary>åŸè®¾è®¡ï¼ˆä»…ä¾›å‚è€ƒï¼Œä¸å®æ–½ï¼‰</summary>

**æ—¶é—´**: 3-5å¤©
</details>

---

## âš ï¸ è®¾è®¡æƒè¡¡

### ä¸ºä»€ä¹ˆå…è®¸ä¸åŒçš„update()ç­¾åï¼Ÿ

**å†³ç­–**: ä¸å¼ºåˆ¶ç»Ÿä¸€updateç­¾åï¼Œå…è®¸ç®—æ³•ç‰¹å®šå‚æ•°

**ç†ç”±**:
- DQNéœ€è¦: `update(batch)` - ä»replay bufferæ‰¹é‡æ›´æ–°
- PPOéœ€è¦: `update(trajectory)` - å®Œæ•´è½¨è¿¹æ›´æ–°
- å¼ºåˆ¶ç»Ÿä¸€ä¼šå¯¼è‡´æ¥å£ä¸è‡ªç„¶

**æ›¿ä»£æ–¹æ¡ˆ** (rejected):
```python
# âŒ æ–¹æ¡ˆ1: å¼ºåˆ¶ç»Ÿä¸€ç­¾å
def update(self, data: Dict) -> float:
    # éœ€è¦åˆ¤æ–­dataæ ¼å¼ï¼Œä¸ç›´è§‚

# âŒ æ–¹æ¡ˆ2: å¤šä¸ªupdateæ–¹æ³•
def update_batch(self, batch): ...
def update_trajectory(self, traj): ...
# æ¥å£å¤æ‚åŒ–
```

**é‡‡ç”¨æ–¹æ¡ˆ** (chosen):
```python
# âœ… æ–¹æ¡ˆ3: çµæ´»ç­¾å
def update(self, *args, **kwargs) -> float:
    # å­ç±»æ ¹æ®éœ€è¦å®šä¹‰å…·ä½“ç­¾å
    # æ–‡æ¡£æ¸…æ¥šè¯´æ˜æ¯ä¸ªç®—æ³•çš„ç­¾å
```

### ä¸ºä»€ä¹ˆä¸ç”¨æŠ½è±¡Factory Patternï¼Ÿ

**å†³ç­–**: ä½¿ç”¨ç®€å•çš„ALGORITHM_REGISTRYå­—å…¸

**ç†ç”±**:
- Registryæ›´ç›´è§‚ï¼Œæ˜“äºæ·»åŠ æ–°ç®—æ³•
- ä¸éœ€è¦å¤æ‚çš„å·¥å‚ç±»å±‚æ¬¡
- é…ç½®é©±åŠ¨ï¼Œæ˜“äºæ‰©å±•

---

## âœ… æˆåŠŸæ ‡å‡†

### Phase 1å®Œæˆæ ‡å‡†
- [ ] BaseAgentæ¥å£å®šä¹‰å®Œæˆ
- [ ] OffPolicyTrainerå®ç°å®Œæˆ
- [ ] DQNAgentç»§æ‰¿BaseAgent
- [ ] `train.py --algorithm dqn` å¯è¿è¡Œ
- [ ] è®­ç»ƒç»“æœä¸æ—§ç‰ˆä¸€è‡´

### Phase 2å®Œæˆæ ‡å‡†
- [ ] OnPolicyTrainerå®ç°å®Œæˆ
- [ ] PPOAgentå®ç°å®Œæˆ
- [ ] PPOè®­ç»ƒæ”¶æ•›
- [ ] Level 1å¯¹æ¯”å®éªŒï¼ˆDQN vs PPOï¼‰å®Œæˆ

### æœ€ç»ˆéªŒæ”¶æ ‡å‡†
- [ ] è‡³å°‘æ”¯æŒ4ç§ç®—æ³•ï¼ˆDQN, Double DQN, PPO, A2Cï¼‰
- [ ] ç»Ÿä¸€çš„`train.py`å…¥å£
- [ ] æ‰€æœ‰ç®—æ³•Level 1æµ‹è¯•é€šè¿‡
- [ ] æ–‡æ¡£å®Œæ•´ï¼ˆæ¯ä¸ªç®—æ³•æœ‰è¯´æ˜ï¼‰
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–æ ¸å¿ƒç»„ä»¶

---

**Date**: 2025-10-19
**Status**: è®¾è®¡å®Œæˆï¼Œå¾…å®æ–½
**Next**: å¼€å§‹Phase 1 - DQNé‡æ„
