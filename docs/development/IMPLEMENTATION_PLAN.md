# Modular RL Framework Implementation Plan

**ç›®æ ‡**: å»ºç«‹åŒ…å« DQN å’Œ Rule-based baselines çš„å®Œæ•´æ¡†æ¶ï¼Œä½œç‚ºæœªä¾†ç®—æ³•å°æ¯”çš„åŸºç¤

**æ€»æ—¶é•¿**: 2-3å‘¨
**ä¼˜å…ˆçº§**: P0ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰

---

## ğŸ“‹ Implementation Roadmap

```
Week 1-2: Phase 1 - DQN Refactoring (Foundation)
    â†“
Week 3: Phase 2 - Rule-based Baselines â­
    â†“
Framework Complete! Ready for algorithm comparison
```

**ç›®æ¨™**: å»ºç«‹é€šç”¨æ¡†æ¶ + å®Œæ•´çš„ baseline é›†åˆï¼ˆ1 å€‹ RL baseline + 3 å€‹ rule-based baselinesï¼‰ï¼Œä½œç‚ºæœªä¾†æ–°ç®—æ³•å°æ¯”çš„åŸºç¤
**è¯¦è§**: [BASELINE_ALGORITHMS.md](../algorithms/BASELINE_ALGORITHMS.md)

---

## ğŸ“… Phase 1: DQN Refactoring (Week 1-2)

**ç›®æ ‡**: é‡æ„ç°æœ‰DQNåˆ°æ–°æ¶æ„ï¼Œä¿æŒåŠŸèƒ½ä¸å˜

### âš ï¸ Critical Requirement: ä¿ç•™ Multi-Level Training

**Multi-Level Training** æ˜¯ç³»ç»Ÿçš„ **Novel Aspect #1**ï¼ˆè§ README.mdï¼‰ï¼Œå¿…é¡»å®Œæ•´ä¿ç•™ã€‚

**è¦æ±‚**:
- âœ… æ‰€æœ‰ 6 ä¸ªè®­ç»ƒçº§åˆ«ï¼ˆLevel 0-5ï¼‰å¿…é¡»åœ¨é‡æ„åå¯ç”¨
- âœ… train.py å¿…é¡»æ”¯æŒ `--level {0,1,2,3,4,5}` å‚æ•°
- âœ… æ—¶é—´ä¼°ç®—å¿…é¡»å‡†ç¡®ï¼ˆLevel 1 â‰ˆ 2h, Level 3 â‰ˆ 10h, Level 5 â‰ˆ 35hï¼‰

### ğŸ¯ Success Criteria
- [ ] BaseAgentæ¥å£å®šä¹‰å®Œæˆ
- [ ] OffPolicyTrainerå®ç°å®Œæˆ
- [ ] DQNAgentç»§æ‰¿BaseAgentå¹¶é€šè¿‡æµ‹è¯•
- [ ] **Multi-Level Training å®Œæ•´ä¿ç•™ï¼ˆP0ï¼‰** â­
- [ ] è®­ç»ƒç»“æœä¸ `train_online_rl.py` ä¸€è‡´ï¼ˆç›¸åŒseedï¼‰
- [ ] `train.py --algorithm dqn --level {0-5}` å…¨éƒ¨å¯è¿è¡Œ â­

### ğŸ“ Detailed Tasks

#### Task 1.1: Create BaseAgent Interface (2-3 hours)

**File**: `src/agents/base_agent.py`

```python
from abc import ABC, abstractmethod

class BaseAgent(ABC):
    @abstractmethod
    def select_action(self, state, deterministic=False):
        pass

    @abstractmethod
    def update(self, *args, **kwargs):
        pass

    @abstractmethod
    def save(self, path):
        pass

    @abstractmethod
    def load(self, path):
        pass
```

**Validation**:
```bash
python -c "from src.agents.base_agent import BaseAgent; print('âœ… Import OK')"
```

---

#### Task 1.2: Create OffPolicyTrainer (4-6 hours)

**File**: `src/trainers/off_policy_trainer.py`

**Implementation**:
- Replay buffer management
- Per-step update logic
- Episode metrics collection

**Test**:
```python
# Unit test
def test_off_policy_trainer():
    env = SatelliteHandoverEnv(...)
    agent = DQNAgent(...)
    trainer = OffPolicyTrainer(env, agent, config)
    metrics = trainer.train_episode(0)
    assert 'reward' in metrics
    assert 'loss' in metrics
```

---

#### Task 1.3: Refactor DQNAgent (6-8 hours)

**Changes**:
```python
# Before (train_online_rl.py)
class DQNAgentV2:
    def __init__(self, state_dim, action_dim, config):
        ...

# After (src/agents/dqn/dqn_agent.py)
from src.agents.base_agent import BaseAgent

class DQNAgent(BaseAgent):
    def __init__(self, obs_space, action_space, config):
        super().__init__()
        self.state_dim = obs_space.shape[0]
        self.action_dim = action_space.n
        ...
```

**Migration Checklist**:
- [ ] ç»§æ‰¿ BaseAgent
- [ ] å®ç° select_action()
- [ ] å®ç° update()
- [ ] å®ç° save/load()
- [ ] ç§»åŠ¨åˆ° `src/agents/dqn/`

---

#### Task 1.4: Implement Multi-Level Training (3-4 hours) â­ P0

**Priority**: P0 - CRITICAL (Novel Aspect #1)

**Goal**: ä¿ç•™ 6 ä¸ªè®­ç»ƒçº§åˆ«ï¼Œç¡®ä¿é‡æ„åå®Œæ•´å¯ç”¨

**File 1**: `src/configs/training_levels.py`

```python
"""
Multi-Level Training Strategy Configuration

Novel Aspect #1 from README.md - must be preserved during refactoring.
"""

TRAINING_LEVELS = {
    0: {
        'name': 'Smoke Test',
        'num_satellites': 10,
        'num_episodes': 10,
        'estimated_time': '10 minutes',
        'description': 'System verification - code runs without errors',
        'use_case': 'Debug, deployment verification',
    },
    1: {
        'name': 'Quick Validation',
        'num_satellites': 20,
        'num_episodes': 100,
        'estimated_time': '2 hours',
        'description': 'Verify training logic, observe learning curve',
        'use_case': 'Fast idea validation, hyperparameter testing',
    },
    2: {
        'name': 'Development',
        'num_satellites': 50,
        'num_episodes': 300,
        'estimated_time': '6 hours',
        'description': 'Debug hyperparameters and reward functions',
        'use_case': 'Development iteration',
    },
    3: {
        'name': 'Validation',
        'num_satellites': 101,
        'num_episodes': 500,
        'estimated_time': '10 hours',
        'description': 'Validate effectiveness - recommended starting point',
        'use_case': 'Paper draft experiments',
        'recommended': True,
    },
    4: {
        'name': 'Baseline',
        'num_satellites': 101,
        'num_episodes': 1000,
        'estimated_time': '21 hours',
        'description': 'Establish stable baseline',
        'use_case': 'Paper experiments',
    },
    5: {
        'name': 'Full Training',
        'num_satellites': 101,
        'num_episodes': 1700,
        'estimated_time': '35 hours',
        'description': 'Complete training',
        'use_case': 'Final paper experiments',
    },
}

def get_level_config(level: int) -> dict:
    """Get configuration for specific training level"""
    if level not in TRAINING_LEVELS:
        raise ValueError(f"Invalid level {level}. Must be 0-5.")
    return TRAINING_LEVELS[level]
```

**File 2**: Update `train.py` to support `--level`

```python
# In train.py
from src.configs.training_levels import get_level_config

parser.add_argument('--level', type=int, choices=[0,1,2,3,4,5],
                   required=True,
                   help='Training level (0=smoke test, 5=full training)')

# Apply level config
level_config = get_level_config(args.level)
print(f"Training Level {args.level}: {level_config['name']}")
print(f"  Estimated time: {level_config['estimated_time']}")
print(f"  Satellites: {level_config['num_satellites']}")
print(f"  Episodes: {level_config['num_episodes']}")

config['training'].update({
    'num_satellites': level_config['num_satellites'],
    'num_episodes': level_config['num_episodes'],
})
```

**Validation**:
```bash
# Test all 6 levels
for level in 0 1 2 3 4 5; do
    echo "Testing Level $level..."
    python train.py --algorithm dqn --level $level --output-dir output/test_level_$level
done
```

**Success Criteria**:
- [ ] `src/configs/training_levels.py` created with all 6 levels
- [ ] `train.py --level {0-5}` all executable
- [ ] Time estimates accurate (validate Level 1 â‰ˆ 2h)
- [ ] Config correctly applied (check num_satellites, num_episodes)

---

#### Task 1.5: Create Unified train.py (4-6 hours)

**File**: `train.py`

**Features**:
- Algorithm registry (ALGORITHM_REGISTRY)
- Config loading (algorithm + training_level)
- Unified training loop
- Logging and checkpointing

**Test**:
```bash
# Test with DQN
python train.py --algorithm dqn --level 0 --output-dir output/test_dqn
```

---

#### Task 1.6: Validation & Comparison (4-6 hours)

**Goal**: éªŒè¯é‡æ„ååŠŸèƒ½å®Œæ•´ï¼ŒåŒ…æ‹¬ç®—æ³•æ­£ç¡®æ€§å’Œ multi-level åŠŸèƒ½

**Validation Script**: `scripts/validate_dqn_refactor.py`

```python
import torch
import numpy as np

def compare_training_results():
    """Compare old vs new DQN training"""
    # Train with old system
    old_rewards = train_old_dqn(seed=42, episodes=100)

    # Train with new system
    new_rewards = train_new_dqn(seed=42, episodes=100)

    # Statistical comparison
    assert np.abs(np.mean(old_rewards - new_rewards)) < 1.0
    print("âœ… Refactoring validation passed")

def validate_multi_level():
    """Validate all 6 training levels"""
    import time

    for level in [0, 1, 3]:  # Test critical levels
        print(f"Testing Level {level}...")
        start_time = time.time()

        # Run training
        os.system(f"python train.py --algorithm dqn --level {level} "
                  f"--output-dir output/validate_level_{level}")

        elapsed = (time.time() - start_time) / 3600  # hours

        # Check time estimates
        expected_times = {0: 0.17, 1: 2.0, 3: 10.0}  # hours
        tolerance = 0.2  # 20%

        assert abs(elapsed - expected_times[level]) / expected_times[level] < tolerance, \
            f"Level {level} time mismatch: {elapsed:.1f}h vs {expected_times[level]}h"

        print(f"âœ… Level {level} validated: {elapsed:.1f}h")
```

**Acceptance Test**:
- [ ] Algorithm: Same seed â†’ same reward curve (Â±5% tolerance)
- [ ] Algorithm: Same loss curve trend
- [ ] Algorithm: Checkpoint compatibility (can load old DQN models)
- [ ] **Multi-Level: All 6 levels executable** â­
- [ ] **Multi-Level: Time estimates accurate (Level 1 â‰ˆ 2h)** â­
- [ ] **Multi-Level: Config correctly applied** â­

---

### â° Phase 1 Timeline

| Task | Duration | Dependencies | Deliverable |
|------|----------|--------------|-------------|
| 1.1 BaseAgent | 2-3h | None | base_agent.py |
| 1.2 OffPolicyTrainer | 4-6h | Task 1.1 | off_policy_trainer.py |
| 1.3 Refactor DQNAgent | 6-8h | Task 1.1, 1.2 | dqn/dqn_agent.py |
| **1.4 Multi-Level (P0)** â­ | **3-4h** | **None** | **training_levels.py** |
| 1.5 Unified train.py | 4-6h | Task 1.3, 1.4 | train.py |
| 1.6 Validation | 4-6h | All above | Validated refactor |
| 1.4 train.py | 4-6h | Task 1.3 | train.py |
| 1.5 Validation | 2-4h | Task 1.4 | validation_report.md |
| **Total** | **18-27h** | | **~3-4 days** |

---

## ğŸ“… Phase 2: Rule-based Baselines (Week 3)

**ç›®æ ‡**: å¯¦ç¾ rule-based handover baselinesï¼Œå»ºç«‹å®Œæ•´çš„å°æ¯”æ¡†æ¶

**Priority**: P0 (æ¡†æ¶å¿…éœ€)

**æ™‚é–“**: 1.5 days (7.5-9.5 hours)

**ğŸ“– è©³ç´°æ–‡æª”**: [PHASE2_RULE_BASED_METHODS.md](PHASE2_RULE_BASED_METHODS.md)

---

### ğŸ“ Tasks Summary

æœ¬ Phase è©³ç´°å…§å®¹è«‹åƒè€ƒç¨ç«‹æ–‡æª” [PHASE2_RULE_BASED_METHODS.md](PHASE2_RULE_BASED_METHODS.md)ã€‚

**Task 2.1**: Base Strategy Protocol (1h)
- å®šç¾© `HandoverStrategy` Protocol
- Duck typing è¨­è¨ˆï¼ˆä¸ç¹¼æ‰¿ BaseAgentï¼‰
- èˆ‡ RL framework zero coupling

**Task 2.2**: Implement 3 Strategies (3-4.5h)
1. **Strongest RSRP** (15 min) - ç°¡å–®å•Ÿç™¼å¼ç­–ç•¥
2. **A4-based Strategy** (30 min) - åŸºæ–¼ 3GPP A4 Event çš„å®Œæ•´ç­–ç•¥ï¼ŒYu 2022 é©—è­‰
3. **D2-based Strategy** (45 min) - åŸºæ–¼ 3GPP D2 Event çš„ NTN å°ˆç”¨ç­–ç•¥ â­

**Note**: A4/D2 Event æ˜¯ 3GPP æ¸¬é‡å ±å‘Šè§¸ç™¼æ¢ä»¶ï¼Œæˆ‘å€‘è£œå……é¸æ“‡é‚è¼¯å’Œåˆ‡æ›æ±ºç­–

**Task 2.3**: Unified Evaluation Framework (2h)
- çµ±ä¸€æ¥å£ `evaluate_strategy()`
- æ”¯æŒ RL å’Œ rule-based æ–¹æ³•
- Level 1 å¿«é€Ÿæ¸¬è©¦

**Task 2.4**: Level 1 Comparison (2-3h)
- å°æ¯” 4 å€‹æ–¹æ³•ï¼š
  - **RL**: DQN
  - **Rule-based**: Strongest RSRP, A4-based Strategy, D2-based Strategy
- ç”Ÿæˆå°æ¯”å ±å‘Šï¼Œå»ºç«‹ baseline æ¡†æ¶

---

### ğŸ¯ Success Criteria
- [ ] 3 å€‹ rule-based baselines å¯¦ç¾å®Œæˆ
- [ ] Duck typing è¨­è¨ˆï¼ˆä¸ä¾è³´ BaseAgentï¼‰
- [ ] Level 1 æ¸¬è©¦é€šéï¼ˆæ‰€æœ‰ 4 å€‹æ–¹æ³•å¯é‹è¡Œï¼‰
- [ ] çµ±ä¸€è©•ä¼°æ¡†æ¶å®Œæˆï¼ˆæ”¯æŒ RL + rule-basedï¼‰
- [ ] **Baseline æ¡†æ¶å®Œæ•´ï¼Œå¯ç”¨æ–¼æœªä¾†ç®—æ³•å°æ¯”** â­

---

### â° Phase 2 Timeline

| Task | Duration | Dependencies | Deliverable |
|------|----------|--------------|-------------|
| 2.1 Base Protocol | 1h | Phase 1 | base_strategy.py |
| 2.2 3 Strategies | 3-4.5h | Task 2.1 | 3 strategy implementations |
| 2.3 Evaluation Framework | 2h | Task 2.2 | evaluate.py |
| 2.4 Level 1 Comparison | 2-3h | Task 2.3 | comparison_report.md |
| **Total** | **7.5-9.5h** | | **~1.5 days**

---

## ğŸ“Š Milestone Checklist

### After Phase 1
- [ ] BaseAgent interface finalized
- [ ] OffPolicyTrainer tested
- [ ] DQNAgent refactored and validated
- [ ] **Multi-Level Training (Level 0-5) working** â­
- [ ] train.py working with `--algorithm dqn --level {0-5}`
- [ ] Documentation updated

### After Phase 2 â­ COMPLETE
- [ ] 3 rule-based baselines implemented
- [ ] Unified evaluation framework working
- [ ] Level 1 baseline evaluation (1 RL + 3 rule-based) completed
- [ ] **Baseline framework complete and ready for algorithm comparison** â­

### Final Deliverables
- [ ] Unified train.py supporting DQN
- [ ] 3 rule-based baselines (Strongest RSRP, A4-based, D2-based)
- [ ] Unified evaluation framework (RL + rule-based)
- [ ] Complete documentation
- [ ] Unit tests for all components
- [ ] **Framework ready for future algorithm comparison** â­

---

## ğŸ”§ Development Guidelines

### Code Quality Standards
- **Type hints**: All functions
- **Docstrings**: Google style
- **Tests**: Pytest for all agents
- **Linting**: Black + flake8

### Git Workflow
```bash
# Feature branch for each phase
git checkout -b phase1/dqn-refactor
git checkout -b phase2/ppo-implementation
git checkout -b phase3/dqn-variants
```

### Testing Strategy
- **Unit tests**: Each agent class
- **Integration tests**: Full training loop
- **Regression tests**: Compare with old DQN
- **Level 0 smoke test**: All new algorithms

---

## ğŸš¨ Risk Mitigation

### Risk 1: Refactored DQN doesn't match old performance
**Mitigation**:
- Use exact same hyperparameters
- Compare with frozen random seeds
- Debug with small-scale tests first

### Risk 2: PPO doesn't converge
**Mitigation**:
- Start with proven hyperparameters from literature
- Test on simpler environments first (CartPole)
- Consult Stable-Baselines3 implementation

### Risk 3: Timelineå»¶æœŸ
**Mitigation**:
- Phase 1-2 are complete project scope (no Phase 3-4)
- Minimum viable: DQN + 3 rule-based baselines
- Can reduce Level 1 experiment scope if time-constrained
- Track progress with regular checkpoints

---

## ğŸ“ˆ Success Metrics

### Quantitative
- [ ] All algorithms converge within expected episodes
- [ ] PPO shows lower variance than DQN (literature validation)
- [ ] Code coverage > 80%
- [ ] Training time per episode < 5s

### Qualitative
- [ ] Code is modular and extensible
- [ ] Documentation is clear
- [ ] Easy to add new algorithms (validated by Phase 2-3)
- [ ] Team consensus on architecture

---

## ğŸ“š Reference Implementations

**Study these for inspiration** (don't copy directly):
1. **Stable-Baselines3**: https://github.com/DLR-RM/stable-baselines3
   - Best practices for RL implementations
2. **CleanRL**: https://github.com/vwxyzjn/cleanrl
   - Simple, educational implementations
3. **Tianshou**: https://github.com/thu-ml/tianshou
   - Modular RL framework design

---

**Created**: 2025-10-19
**Last Updated**: 2025-10-20 (å»ºç«‹ Baseline æ¡†æ¶)
**Status**: Planning Complete, Ready to Start Phase 1
**Estimated Total Time**:
- Phase 1 (DQN Refactoring): 2 weeks
- Phase 2 (Rule-based Baselines): 1.5 days
- **Total**: **2-3 weeks** â­

**Next Action**: Start Task 1.1 - Create BaseAgent Interface

**ç›®æ¨™**: å»ºç«‹åŒ…å« DQN (RL baseline) å’Œ 3 å€‹ rule-based baselines çš„å®Œæ•´æ¡†æ¶ï¼Œä½œç‚ºæœªä¾†ç®—æ³•å°æ¯”çš„åŸºç¤ã€‚

**Note**: Phase 2 (Rule-based Baselines) å®Œå…¨ç¨ç«‹æ–¼ RL æ¡†æ¶ï¼Œå¯¦ç¾å¿«é€Ÿï¼ˆ1.5 å¤©ï¼‰ã€‚æ¡†æ¶å®Œæˆå¾Œå¯è¼•é¬†åŠ å…¥æ–°ç®—æ³•é€²è¡Œå°æ¯”ã€‚
