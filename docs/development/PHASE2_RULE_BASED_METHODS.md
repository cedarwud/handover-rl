# Phase 2: Rule-based Comparison Methods

**é€±æ¬¡**: Week 3
**æ™‚é–“**: 1.5 days (7.5-9.5 hours)
**å„ªå…ˆç´š**: P1 (è«–æ–‡å¿…éœ€)
**ä¾è³´**: Phase 1 (DQN Refactoring) å®Œæˆ

---

## ğŸ“‹ Overview

### ç›®æ¨™
å¯¦ç¾ rule-based handover baselinesï¼Œå»ºç«‹å®Œæ•´çš„å°æ¯”æ¡†æ¶ï¼Œä½œç‚ºæœªä¾†ç®—æ³•è©•ä¼°çš„åŸºç¤ã€‚

### æ ¸å¿ƒåƒ¹å€¼
- å»ºç«‹æ¨™æº–çš„ rule-based baselinesï¼ˆStrongest RSRP, A4-based Strategy, D2-based Strategyï¼‰
- æä¾›çµ±ä¸€çš„è©•ä¼°æ¡†æ¶ï¼ˆæ”¯æŒ RL å’Œ rule-based æ–¹æ³•ï¼‰
- ç‚ºæœªä¾†çš„ç®—æ³•å°æ¯”æä¾›å®Œæ•´çš„ baseline é›†åˆ
- ä½¿ç”¨ NTN å°ˆç”¨çš„ D2-based Strategyï¼ˆåŸºæ–¼ 3GPP D2 Eventï¼‰ï¼Œçªé¡¯ NTN ç‰¹æ€§ â­

**é‡è¦è¯´æ˜**: A4/D2 Event æ˜¯ 3GPP å®šä¹‰çš„æµ‹é‡æŠ¥å‘Šè§¦å‘æ¡ä»¶ã€‚ä½œä¸º baseline strategyï¼Œæˆ‘ä»¬è¡¥å……äº†é€‰æ‹©é€»è¾‘å’Œåˆ‡æ¢å†³ç­–ã€‚

### è¨­è¨ˆåŸå‰‡
- âœ… **å®Œå…¨ç¨ç«‹**ï¼šä¸ä¾è³´ RL æ¡†æ¶å¯¦ç¾ï¼ˆåªä¾è³´ç’°å¢ƒ APIï¼‰
- âœ… **Duck Typing**ï¼šé€šéçµ±ä¸€ä»‹é¢è©•ä¼°ï¼Œä¸å¼·åˆ¶ç¹¼æ‰¿
- âœ… **å­¸è¡“åš´è¬¹**ï¼šæ‰€æœ‰åƒæ•¸éƒ½æœ‰ SOURCE å¼•ç”¨ï¼ˆ3GPP + orbit-engineï¼‰
- âœ… **å¿«é€Ÿå¯¦ç¾**ï¼šç¸½æ™‚é–“ 1.5 å¤©

---

## ğŸ¯ Success Criteria

### Implementation
- [ ] 3 ç¨® rule-based baselines å¯¦ç¾å®Œæˆ
- [ ] Config files created with proper SOURCE citations
- [ ] Evaluation script supports both RL and rule-based
- [ ] Unit tests pass for all baselines

### Experiments
- [ ] Level 1 evaluation completed for all baselines
- [ ] Unified evaluation framework working correctly
- [ ] Baseline comparison report generated

### Documentation
- [ ] docs/strategies/RULE_BASED_METHODS.md created
- [ ] Config README with parameter explanations
- [ ] **Framework ready for future algorithm comparison** â­

---

## ğŸ“ Detailed Tasks

### Task 2.1: Base Strategy Protocol (1 hour)

**File**: `src/strategies/base_strategy.py`

**è¨­è¨ˆåŸå‰‡**:
- âŒ ä¸ç¹¼æ‰¿ BaseAgentï¼ˆé¿å…ä¸å¿…è¦è€¦åˆï¼‰
- âœ… ç°¡å–® Protocolï¼ˆåªå®šç¾©å¿…è¦ä»‹é¢ï¼‰
- âœ… Duck typingï¼ˆçµ±ä¸€è©•ä¼°ï¼Œåˆ†é›¢å¯¦ç¾ï¼‰

**Implementation**:
```python
"""
Rule-based Handover Strategies

These are NOT RL agents. They use fixed rules to make handover decisions.
Used for comparison to demonstrate the value of RL approaches.
"""

from typing import Protocol
import numpy as np

class HandoverStrategy(Protocol):
    """
    Protocol for handover decision strategies.

    Both RL agents and rule-based methods can implement this interface
    through duck typing (no inheritance required).
    """

    def select_action(self, observation: np.ndarray, **kwargs) -> int:
        """
        Select handover action based on observation.

        Args:
            observation: (K, 12) array from environment
            **kwargs: Optional info (e.g., serving_satellite_idx for D2)

        Returns:
            action: int (0 = stay, 1-K = handover to satellite i-1)
        """
        ...
```

**Validation**:
```bash
python -c "from src.strategies.base_strategy import HandoverStrategy; print('âœ… Import OK')"
```

---

### Task 2.2: Implement 3 Rule-based Strategies (1.5 hours)

#### 2.2.1: Strongest RSRP (15 min)

**File**: `src/strategies/strongest_rsrp.py`

**Logic**: Always select satellite with highest RSRP (simplest heuristic)

**Implementation**:
```python
class StrongestRSRPStrategy:
    """
    Strongest RSRP Handover Strategyï¼ˆç®€å•å¯å‘å¼ç­–ç•¥ï¼‰

    Decision rule:
      1. æ‰¾åˆ° RSRP æœ€å¼ºçš„å«æ˜Ÿ
      2. å¦‚æœæœ€å¼ºçš„ä¸æ˜¯å½“å‰æœåŠ¡å«æ˜Ÿï¼Œåˆ‡æ¢
      3. å¦‚æœæœ€å¼ºçš„å°±æ˜¯å½“å‰æœåŠ¡å«æ˜Ÿï¼Œä¿æŒè¿æ¥ï¼ˆstayï¼‰

    Use case: Baseline lower bound for demonstrating improvements.
    Expected performance: High handover rate, high ping-pong.
    """

    def __init__(self):
        pass  # No parameters needed

    def select_action(self, observation: np.ndarray,
                     serving_satellite_idx: int = None,
                     **kwargs) -> int:
        """
        Args:
            observation: (K, 12) array, Index 0 = RSRP (dBm)
            serving_satellite_idx: Current serving satellite index (0-based)

        Returns:
            action: 0 (stay) or 1-K (handover to satellite i-1)
        """
        rsrp_values = observation[:, 0]  # Extract RSRP column
        best_idx = np.argmax(rsrp_values)

        # If best is current serving satellite, stay
        if serving_satellite_idx is not None and best_idx == serving_satellite_idx:
            return 0  # Stay
        else:
            return int(best_idx + 1)  # Handover to best RSRP satellite
```

**Config**: `config/strategies/strongest_rsrp.yaml`
```yaml
strategy: strongest_rsrp
name: "Strongest RSRP"
type: heuristic
description: "Always select satellite with highest RSRP"

parameters: {}  # No parameters

metadata:
  complexity: "O(K)"
  expected_ho_rate: "8-10%"
  expected_ping_pong: "10-15%"
```

---

#### 2.2.2: A4-based Strategy (30 min)

**File**: `src/strategies/a4_based_strategy.py`

**Logic**: A4-based handover strategy (3GPP A4 Event + RSRP selection + handover decision)

**Implementation**:
```python
class A4BasedStrategy:
    """
    A4-based Handover Strategyï¼ˆåŸºäº 3GPP A4 Event çš„å®Œæ•´åˆ‡æ¢ç­–ç•¥ï¼‰

    **3GPP A4 Event å®šä¹‰**:
    - è§¦å‘æ¡ä»¶: Mn + Ofn + Ocn - Hys > Threshï¼ˆé‚»å±… RSRP è¶…è¿‡é˜ˆå€¼ï¼‰
    - æ¥æº: 3GPP TS 38.331 Section 5.5.4.5
    - æ³¨æ„: A4 Event æœ¬èº«åªæ˜¯æµ‹é‡æŠ¥å‘Šè§¦å‘æ¡ä»¶

    **ä½œä¸º Baseline Strategy çš„è¡¥å……**:
    - é€‰æ‹©é€»è¾‘: ä»æ»¡è¶³ A4 æ¡ä»¶çš„å€™é€‰ä¸­é€‰æ‹© RSRP æœ€å¼ºçš„
    - åˆ‡æ¢å†³ç­–: å¦‚æœå€™é€‰æ¯”å½“å‰æœåŠ¡å«æ˜Ÿæ›´å¥½ï¼Œæ‰æ‰§è¡Œåˆ‡æ¢
    - é˜² ping-pong: ä½¿ç”¨ hysteresis å‚æ•°

    Decision rule:
      1. æ£€æŸ¥æ‰€æœ‰é‚»å±…å«æ˜Ÿæ˜¯å¦æ»¡è¶³: RSRP - Hys > Thresh
      2. ä»æ»¡è¶³æ¡ä»¶çš„å€™é€‰ä¸­é€‰æ‹© RSRP æœ€å¼ºçš„
      3. å¦‚æœå€™é€‰æ¯”å½“å‰æœåŠ¡å«æ˜Ÿæ›´å¥½ï¼Œåˆ‡æ¢
      4. å¦åˆ™ä¿æŒå½“å‰è¿æ¥ï¼ˆstayï¼‰

    SOURCE: Yu et al. 2022 - proved A4 > A3 for LEO NTN
    STANDARDS: 3GPP TS 38.331 v18.5.1 Section 5.5.4.5
    """

    def __init__(self, threshold_dbm: float = -100.0,
                 hysteresis_db: float = 1.5,
                 offset_db: float = 0.0):
        """
        Args:
            threshold_dbm: A4 RSRP threshold (SOURCE: Yu 2022 optimal = -100)
            hysteresis_db: Hysteresis parameter (SOURCE: 3GPP typical = 1.5)
            offset_db: Measurement offset (default 0)
        """
        self.threshold = threshold_dbm
        self.hysteresis = hysteresis_db
        self.offset = offset_db

    def select_action(self, observation: np.ndarray,
                     serving_satellite_idx: int = None,
                     **kwargs) -> int:
        """
        A4 Event trigger condition:
          Mn + Ofn + Ocn - Hys > Thresh

        Simplified (no cell-specific offsets):
          neighbor_RSRP - Hys > Thresh

        Args:
            observation: (K, 12) array, Index 0 = RSRP (dBm)
            serving_satellite_idx: Current serving satellite index (0-based)

        Returns:
            action: 0 (stay) or 1-K (handover to satellite i-1)
        """
        rsrp_values = observation[:, 0]

        # Find all neighbors that satisfy A4 trigger condition (exclude current)
        candidates = []
        for i, rsrp in enumerate(rsrp_values):
            if serving_satellite_idx is None or i != serving_satellite_idx:
                # A4 trigger check
                trigger_value = rsrp + self.offset - self.hysteresis
                if trigger_value > self.threshold:
                    candidates.append((i, rsrp))

        if candidates:
            # Select best RSRP from candidates
            best_candidate = max(candidates, key=lambda x: x[1])
            return int(best_candidate[0] + 1)  # Handover to best candidate
        else:
            return 0  # Stay (no A4 event triggered)
```

**Config**: `config/strategies/a4_based.yaml`
```yaml
strategy: a4_based
name: "A4-based Strategy"
type: rule_based_3gpp
description: "3GPP A4 event + RSRP selection + handover decision"

parameters:
  threshold_dbm: -100.0   # SOURCE: Yu et al. 2022 (optimal for LEO)
  hysteresis_db: 1.5      # SOURCE: 3GPP TS 38.331
  offset_db: 0.0

metadata:
  reference: "Yu et al. 2022 - Performance Evaluation of Handover using A4 Event"
  standards: "3GPP TS 38.331 v18.5.1 Section 5.5.4.5"
  trigger_rate: "54.4%"   # From orbit-engine real data
  optimal_for: "LEO NTN (RSRP variation < 10 dB)"
```

---

#### 2.2.3: D2-based Strategy (45 min)

**File**: `src/strategies/d2_based_strategy.py`

**Logic**: D2-based handover strategy (3GPP D2 Event + distance selection + handover decision)

**Implementation**:
```python
class D2BasedStrategy:
    """
    D2-based Handover Strategyï¼ˆåŸºäº 3GPP D2 Event çš„å®Œæ•´åˆ‡æ¢ç­–ç•¥ï¼‰

    **3GPP D2 Event å®šä¹‰**:
    - è§¦å‘æ¡ä»¶ 1: Ml1 - Hys > Thresh1ï¼ˆæœåŠ¡å«æ˜Ÿå¤ªè¿œï¼‰
    - è§¦å‘æ¡ä»¶ 2: Ml2 + Hys < Thresh2ï¼ˆé‚»å±…å«æ˜Ÿå¤Ÿè¿‘ï¼‰
    - æ¥æº: 3GPP TS 38.331 Section 5.5.4.15a (Rel-17 NTN)
    - æ³¨æ„: D2 Event æœ¬èº«åªæ˜¯æµ‹é‡æŠ¥å‘Šè§¦å‘æ¡ä»¶

    **ä½œä¸º Baseline Strategy çš„è¡¥å……**:
    - é€‰æ‹©é€»è¾‘: ä»æ»¡è¶³ D2 æ¡ä»¶çš„å€™é€‰ä¸­é€‰æ‹©è·ç¦»æœ€è¿‘çš„
    - åˆ‡æ¢å†³ç­–: å¦‚æœå€™é€‰æ¯”å½“å‰æœåŠ¡å«æ˜Ÿæ›´è¿‘ï¼Œæ‰æ‰§è¡Œåˆ‡æ¢
    - NTN ä¸“ç”¨: è€ƒè™‘å«æ˜Ÿç§»åŠ¨ç‰¹æ€§ï¼ˆè·ç¦»è€Œéä¿¡å·å¼ºåº¦ï¼‰

    Decision rule:
      1. æ£€æŸ¥å½“å‰æœåŠ¡å«æ˜Ÿè·ç¦»æ˜¯å¦ > threshold1ï¼ˆå¤ªè¿œï¼‰
      2. æ‰¾åˆ°æ‰€æœ‰è·ç¦» < threshold2 çš„å€™é€‰å«æ˜Ÿï¼ˆå¤Ÿè¿‘ï¼‰
      3. ä»å€™é€‰ä¸­é€‰æ‹©è·ç¦»æœ€è¿‘çš„
      4. å¦‚æœå€™é€‰è·ç¦» < å½“å‰æœåŠ¡å«æ˜Ÿè·ç¦»ï¼Œåˆ‡æ¢
      5. å¦åˆ™ä¿æŒå½“å‰è¿æ¥ï¼ˆstayï¼‰

    SOURCE: orbit-engine real data analysis (71-day TLE)
    STANDARDS: 3GPP TS 38.331 v18.5.1 Section 5.5.4.15a
    """

    def __init__(self, threshold1_km: float = 1412.8,
                 threshold2_km: float = 1005.8,
                 hysteresis_km: float = 50.0):
        """
        Args:
            threshold1_km: Serving satellite too far (SOURCE: orbit-engine Stage 4)
            threshold2_km: Neighbor close enough (SOURCE: orbit-engine Stage 4)
            hysteresis_km: Distance hysteresis (SOURCE: 3GPP typical)
        """
        self.threshold1 = threshold1_km
        self.threshold2 = threshold2_km
        self.hysteresis = hysteresis_km

    def select_action(self, observation: np.ndarray,
                     serving_satellite_idx: int = None,
                     **kwargs) -> int:
        """
        D2 Event trigger condition:
          (serving_dist - Hys > Thresh1) AND (neighbor_dist + Hys < Thresh2)

        Args:
            observation: (K, 12) array, Index 3 = Distance (km)
            serving_satellite_idx: Current serving satellite index (0-based)

        Returns:
            action: 0 (stay) or 1-K (handover to satellite i-1)
        """
        distances = observation[:, 3]  # Extract distance column

        # Get serving satellite distance
        if serving_satellite_idx is not None and serving_satellite_idx < len(distances):
            serving_distance = distances[serving_satellite_idx]
        else:
            serving_distance = np.inf  # If unknown, assume far

        # D2 Condition 1: Serving satellite too far?
        if serving_distance - self.hysteresis > self.threshold1:
            # D2 Condition 2: Find all neighbors close enough
            candidates = []
            for i, dist in enumerate(distances):
                if serving_satellite_idx is None or i != serving_satellite_idx:
                    if dist + self.hysteresis < self.threshold2:
                        candidates.append((i, dist))

            if candidates:
                # Select closest from candidates
                best_candidate = min(candidates, key=lambda x: x[1])
                return int(best_candidate[0] + 1)  # Handover to closest

        return 0  # Stay (no D2 event triggered or no suitable candidate)
```

**Config**: `config/strategies/d2_based.yaml`
```yaml
strategy: d2_based
name: "D2-based Strategy"
type: rule_based_ntn
description: "3GPP Rel-17 D2 event + distance selection + handover decision (NTN-specific)"

parameters:
  threshold1_km: 1412.8   # SOURCE: orbit-engine Stage 4 dynamic analysis
  threshold2_km: 1005.8   # SOURCE: orbit-engine Stage 4 dynamic analysis
  hysteresis_km: 50.0     # SOURCE: 3GPP typical

metadata:
  reference: "3GPP Rel-17 NTN standardization"
  standards: "3GPP TS 38.331 v18.5.1 Section 5.5.4.15a"
  trigger_rate: "6.5%"    # From orbit-engine real data
  optimal_for: "LEO NTN extreme scenarios (serving satellite too far)"
  novelty: "First use of D2 event in RL baseline comparison research"
```

**D2 Threshold æ¨å¯¼è¯´æ˜**:

**æ•°æ®æº**: orbit-engine Stage 4 - 71å¤©çœŸå® TLE æ•°æ®åˆ†æï¼ˆ2025-07-27 to 2025-10-17ï¼‰

**æ–¹æ³•**:
1. **è·ç¦»åˆ†å¸ƒç»Ÿè®¡**:
   - åˆ†æ 101 Starlink satellites åˆ°åœ°é¢ç”¨æˆ·çš„è·ç¦»èŒƒå›´
   - Starlink orbit: 550km altitudeï¼ˆorbital distance: ~900-1500kmï¼‰

2. **threshold1ï¼ˆæœåŠ¡å«æ˜Ÿ"å¤ªè¿œ"ï¼‰**:
   - å®šä¹‰ï¼šå½“å‰æœåŠ¡å«æ˜Ÿè·ç¦»è¶…è¿‡æ­¤å€¼æ—¶è€ƒè™‘åˆ‡æ¢
   - å€¼ï¼š1412.8 km
   - åŸºäºï¼šè·ç¦»åˆ†å¸ƒçš„ä¸Šå››åˆ†ä½æ•°ï¼ˆ75th percentileï¼‰
   - å«ä¹‰ï¼šæœåŠ¡å«æ˜Ÿæ¥è¿‘å¯è§èŒƒå›´è¾¹ç¼˜

3. **threshold2ï¼ˆé‚»å±…å«æ˜Ÿ"å¤Ÿè¿‘"ï¼‰**:
   - å®šä¹‰ï¼šé‚»å±…å«æ˜Ÿè·ç¦»å°äºæ­¤å€¼æ—¶å¯ä½œä¸ºåˆ‡æ¢å€™é€‰
   - å€¼ï¼š1005.8 km
   - åŸºäºï¼šè·ç¦»åˆ†å¸ƒçš„ä¸­ä½æ•°ï¼ˆ50th percentileï¼‰
   - å«ä¹‰ï¼šé‚»å±…å«æ˜Ÿä½äºè¾ƒä¼˜æœåŠ¡ä½ç½®

**éªŒè¯**:
- âœ… Trigger rate: ~6.5%ï¼ˆä¸ 3GPP CHO å…¸å‹å€¼ä¸€è‡´ï¼‰
- âœ… é¿å…è¿‡äºé¢‘ç¹åˆ‡æ¢ï¼ˆthreshold1 - threshold2 = 407km hysteresis marginï¼‰
- âœ… åŸºäºçœŸå®è½¨é“æ•°æ®ï¼Œä¸æ˜¯ä¼°ç®—å€¼

**Note**: è¿™äº›å€¼æ¥è‡ª orbit-engine Stage 4 çš„çœŸå®å«æ˜Ÿè½¨é“åˆ†æï¼Œç¬¦åˆ"REAL ALGORITHMS ONLY"åŸåˆ™

---

### Task 2.3: Evaluation Framework (3-4 hours)

**File**: `scripts/evaluate_strategies.py`

**Goal**: çµ±ä¸€è©•ä¼° RL agents å’Œ rule-based strategies

**Implementation**:
```python
"""
Unified Strategy Evaluation Framework

Supports both RL agents and rule-based strategies through duck typing.
Only requirement: object must have select_action(observation) method.
"""

import numpy as np
from typing import Union, Dict, List
from src.environments import SatelliteHandoverEnv

def evaluate_strategy(
    strategy,  # RL agent or rule-based strategy
    env: SatelliteHandoverEnv,
    num_episodes: int = 100,
    seed: int = 42
) -> Dict[str, float]:
    """
    Evaluate any strategy (RL or rule-based) on environment.

    Args:
        strategy: Any object with select_action(observation) method
        env: SatelliteHandoverEnv instance
        num_episodes: Number of evaluation episodes
        seed: Random seed for reproducibility

    Returns:
        metrics: Dict with performance metrics
    """
    np.random.seed(seed)

    episode_rewards = []
    handover_counts = []
    ping_pong_counts = []
    avg_rsrps = []

    for episode in range(num_episodes):
        obs, info = env.reset(seed=seed + episode)
        done = False
        episode_reward = 0
        handovers = 0

        while not done:
            # Duck typing: works for both RL and rule-based
            action = strategy.select_action(obs)

            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

            episode_reward += reward
            if info.get('handover_occurred', False):
                handovers += 1

        episode_rewards.append(episode_reward)
        handover_counts.append(handovers)
        ping_pong_counts.append(info.get('num_ping_pongs', 0))
        avg_rsrps.append(info.get('avg_rsrp', 0))

    return {
        'avg_reward': np.mean(episode_rewards),
        'std_reward': np.std(episode_rewards),
        'avg_handovers': np.mean(handover_counts),
        'handover_rate_pct': np.mean(handover_counts) / env.episode_duration_minutes * 100,
        'ping_pong_rate_pct': np.mean(ping_pong_counts) / np.mean(handover_counts) * 100,
        'avg_rsrp_dbm': np.mean(avg_rsrps),
    }

def compare_strategies(
    strategies: Dict[str, any],
    env: SatelliteHandoverEnv,
    num_episodes: int = 100
) -> pd.DataFrame:
    """
    Compare multiple strategies and return results table.

    Args:
        strategies: Dict of {name: strategy_object}
        env: Environment
        num_episodes: Evaluation episodes

    Returns:
        DataFrame with comparison results
    """
    results = []

    for name, strategy in strategies.items():
        print(f"Evaluating {name}...")
        metrics = evaluate_strategy(strategy, env, num_episodes)
        metrics['strategy'] = name
        results.append(metrics)

    df = pd.DataFrame(results)
    return df.sort_values('avg_reward', ascending=False)
```

**CLI Usage**:
```bash
# Evaluate single strategy
python scripts/evaluate_strategies.py \
    --strategy-type rule_based \
    --strategy-name a4_event \
    --level 1 \
    --episodes 100

# Compare all strategies
python scripts/evaluate_strategies.py \
    --compare-all \
    --level 1 \
    --output results/level1_comparison.csv
```

---

### Task 2.4: Level 1 Comparison Experiment (2-3 hours)

**Experiment Design**:
```bash
# Run Level 1 evaluation for all strategies
# Total time: ~8-10 hours (parallel recommended)

# Rule-based strategies (fast, ~5 minutes each)
python scripts/evaluate_strategies.py --strategy strongest_rsrp --level 1
python scripts/evaluate_strategies.py --strategy a4_event --level 1
python scripts/evaluate_strategies.py --strategy d2_event --level 1

# RL algorithm (trained model, ~5 minutes for evaluation)
python scripts/evaluate_strategies.py --strategy dqn --level 1 --model output/dqn_level1/best_model.pt

# Generate comparison report
python scripts/compare_results.py \
    --results-dir results/ \
    --output results/level1_comparison_report.md
```

**Expected Output**: `results/level1_comparison_report.md`

```markdown
# Level 1 Strategy Comparison Report

## Performance Metrics

| Strategy | Type | Avg Reward | HO Rate (%) | Avg RSRP (dBm) | Ping-Pong (%) |
|----------|------|------------|-------------|----------------|---------------|
| **DQN** | RL | **-128.7** | **5.8** | **-30.2** | **2.1** |
| Geometry + D2 | NTN Rule | -142.5 | 4.8 | -31.2 | 4.0 |
| RSRP + A4 | 3GPP Rule | -151.2 | 6.1 | -31.5 | 7.5 |
| Strongest RSRP | Heuristic | -168.9 | 8.2 | -32.1 | 12.3 |

## Key Findings

1. **Baseline Performance Hierarchy**:
   - DQN (RL baseline): -128.7
   - D2-based Strategy (NTN-specific): -142.5
   - A4-based Strategy (3GPP standard): -151.2
   - Strongest RSRP (Simple heuristic): -168.9

2. **Rule-based Baseline Ranking**: D2-based > A4-based > Strongest RSRP
   - Validates that NTN-specific design (D2-based) improves performance
   - Confirms Yu et al. 2022 findings (A4 suitable for LEO)

3. **Baseline Framework Complete**:
   - 1 RL baseline + 3 rule-based baselines established
   - Unified evaluation framework working
   - Ready for future algorithm comparison
```

---

## â° Timeline

| Task | Duration | Complexity | Deliverable |
|------|----------|------------|-------------|
| 2.1 Base Strategy Protocol | 1h | Low | base_strategy.py |
| 2.2 Implement 3 Strategies | 1.5h | Low | 3 strategy files + configs |
| 2.3 Evaluation Framework | 3-4h | Medium | evaluate_strategies.py |
| 2.4 Level 1 Comparison | 2-3h | Low | comparison_report.md |
| **Total** | **7.5-9.5h** | | **~1.5 days** |

---

## ğŸ“Š Directory Structure

```
handover-rl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                    # RL algorithms (Phase 1)
â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â””â”€â”€ dqn/
â”‚   â”‚
â”‚   â”œâ”€â”€ strategies/                # Rule-based methods (Phase 2) â­ NEW
â”‚   â”‚   â”œâ”€â”€ base_strategy.py
â”‚   â”‚   â”œâ”€â”€ strongest_rsrp.py
â”‚   â”‚   â”œâ”€â”€ rsrp_a4_event.py
â”‚   â”‚   â””â”€â”€ geometry_d2_event.py
â”‚   â”‚
â”‚   â”œâ”€â”€ trainers/
â”‚   â””â”€â”€ environments/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ algorithms/                # RL configs
â”‚   â””â”€â”€ strategies/                # Rule-based configs â­ NEW
â”‚       â”œâ”€â”€ strongest_rsrp.yaml
â”‚       â”œâ”€â”€ a4_event.yaml
â”‚       â””â”€â”€ d2_event.yaml
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ evaluate_strategies.py    # Unified evaluation â­ NEW
â”‚   â””â”€â”€ compare_results.py        # Comparison analysis â­ NEW
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ development/
    â”‚   â”œâ”€â”€ IMPLEMENTATION_PLAN.md    # Main plan (Phase 1-2)
    â”‚   â””â”€â”€ PHASE2_RULE_BASED_METHODS.md  # This document
    â””â”€â”€ strategies/                # Rule-based docs â­ NEW
        â”œâ”€â”€ RULE_BASED_METHODS.md
        â””â”€â”€ PARAMETER_TUNING.md
```

---

## ğŸ”— Dependencies

### Depends On (Phase 1-3)
- âœ… `SatelliteHandoverEnv` API stable (observation format)
- âŒ **NOT dependent** on RL framework internals

### Environment API Requirements
```python
# Only requirement: environment provides (K, 12) observations
observation, reward, terminated, truncated, info = env.step(action)

# observation shape: (K, 12)
# Index 0: RSRP (dBm)         â† A4 needs this
# Index 3: Distance (km)       â† D2 needs this
```

---

## ğŸ¯ Research Value

### Novel Contributions
1. **First D2-based Strategy as Baseline**: No prior work uses D2 event as basis for RL baseline comparison
2. **NTN-Optimized Parameters**: Thresholds from real TLE data (orbit-engine)
3. **Multi-Dimensional Baseline Framework**: RSRP-based (A4-based) + Distance-based (D2-based)
4. **Complete Strategy Implementation**: Supplement 3GPP events with selection logic and handover decision
4. **Comprehensive Baseline Set**: Covers heuristic, standard, and NTN-specific methods

### Framework Benefits
```
Baseline Framework Includes:
- Simple Heuristic: Strongest RSRP (baseline lower bound)
- 3GPP Standard: A4-based Strategy (A4 event + RSRP selection, validated for LEO)
- NTN-Specific: D2-based Strategy (D2 event + distance selection) â­
- RL Baseline: DQN (standard RL method)

â†’ Provides comprehensive comparison for future algorithms
â†’ Enables fair and thorough evaluation
â†’ Supports reproducible research
```

---

## âœ… Validation Checklist

### Before Starting Phase 2
- [ ] Phase 1 completed (DQN refactored and stable)
- [ ] Environment API confirmed stable
- [ ] Multi-Level Training working (Level 1 available for testing)

### During Implementation
- [ ] Each strategy tested independently
- [ ] Config files validate (YAML syntax)
- [ ] Unit tests pass

### Before Completion
- [ ] All 3 baselines implemented
- [ ] Evaluation framework works for RL + rule-based
- [ ] Level 1 comparison completed
- [ ] **Baseline framework complete and validated** â­

---

**Created**: 2025-10-19
**Last Updated**: 2025-10-20 (å»ºç«‹ Baseline æ¡†æ¶)
**Status**: Planning Complete
**Next Action**: Wait for Phase 1 completion, then start Task 2.1

**ç›®æ¨™**: å»ºç«‹å®Œæ•´çš„ baseline æ¡†æ¶ï¼ˆ1 RL + 3 rule-basedï¼‰ï¼Œä½œç‚ºæœªä¾†ç®—æ³•è©•ä¼°çš„åŸºç¤

**Note**: This phase is completely independent of RL framework. Can be implemented immediately after Phase 1 (DQN refactoring) completion. Framework provides comprehensive baselines for future algorithm comparison.
