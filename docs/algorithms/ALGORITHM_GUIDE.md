# RL ç®—æ³•æ›¿æ›æŒ‡å—

**ç•¶å‰å¯¦ç¾**: DQN (Discrete Action Space)

**æ¶æ§‹å„ªå‹¢**: Environment å®Œå…¨ç¬¦åˆ Gymnasium æ¨™æº– â†’ å¯æ›¿æ›å„ç¨®ç®—æ³•

**æœ€å¾Œæ›´æ–°**: 2025-10-20 (æ˜ç¡®é¡¹ç›®å®æ–½èŒƒå›´)

---

## âš ï¸ IMPORTANT: PROJECT SCOPE CLARIFICATION

**æœ¬é¡¹ç›®å®æ–½èŒƒå›´** (Phase 1-2, 2-3 weeks):
- âœ… **DQN** (å·²å®Œæˆ) - å”¯ä¸€å®æ–½çš„ RL baseline
- âœ… **3 Rule-based baselines**: Strongest RSRP, A4-based Strategy, D2-based Strategy

**æ˜ç¡®è¯´æ˜**:
- âŒ **Tier 2-3 ç®—æ³•ï¼ˆD3QNã€PPOã€A2Cã€Rainbowã€SAC ç­‰ï¼‰ä¸åœ¨å½“å‰é¡¹ç›®å®æ–½èŒƒå›´å†…**
- â­ **æœªæ¥å·¥ä½œ**: ç”¨æˆ·è‡ªå·±çš„ç®—æ³• vs ä»¥ä¸Š 4 ä¸ª baselines (1 RL + 3 Rule-based)

**æœ¬æ–‡æ¡£ç”¨é€”**:
- âœ… Architecture reference (è¯´æ˜ Gymnasium ç¯å¢ƒçš„å¯æ‰©å±•æ€§)
- âœ… Literature review context (äº†è§£é¢†åŸŸç ”ç©¶ç°çŠ¶)
- âŒ NOT a commitment to implement all listed algorithms

è¯¦è§ [BASELINE_ALGORITHMS.md](BASELINE_ALGORITHMS.md) å®Œæ•´è¯´æ˜

---

## ğŸ“š 2023-2025æ–‡ç»èª¿ç ”æ›´æ–°

åŸºæ–¼æœ€æ–°16ç¯‡è«–æ–‡çš„èª¿ç ”çµæœï¼ˆ**ä»…ä¾›äº†è§£é¢†åŸŸç ”ç©¶ç°çŠ¶ï¼Œä¸ä»£è¡¨æœ¬é¡¹ç›®å®æ–½èŒƒå›´**ï¼‰ï¼š

### Tier 1: æœ¬é¡¹ç›®å®æ–½
1. **DQN** â­â­â­â­â­ - å·²å®Œæˆ âœ…
   - ä½¿ç”¨é »ç‡ï¼š62.5%çš„è«–æ–‡
   - æ¨™æº– RL baseline
   - **æœ¬é¡¹ç›®å”¯ä¸€å®æ–½çš„ RL ç®—æ³•**

### Tier 2: æ–‡çŒ®è°ƒç ”å‚è€ƒï¼ˆâŒ ä¸åœ¨æœ¬é¡¹ç›®å®æ–½èŒƒå›´ï¼‰
2. **PPO** â­â­â­â­â­ - **Frontiers 2023æ˜ç¢ºæ¨è–¦**
   - "PPO is the most stable algorithm, converging quickly"
   - 31.3%çš„è«–æ–‡ä½¿ç”¨
   - âŒ ä¸åœ¨æœ¬é¡¹ç›®å®æ–½èŒƒå›´

3. **Double DQN** â­â­â­â­
   - DQNçš„æ¨™æº–æ”¹é€²
   - âŒ ä¸åœ¨æœ¬é¡¹ç›®å®æ–½èŒƒå›´

4. **Dueling DQN** â­â­â­â­
   - âŒ ä¸åœ¨æœ¬é¡¹ç›®å®æ–½èŒƒå›´

5. **D3QN** (Dueling + Double) â­â­â­â­
   - âŒ ä¸åœ¨æœ¬é¡¹ç›®å®æ–½èŒƒå›´

### Tier 3: æ–‡çŒ®è°ƒç ”å‚è€ƒï¼ˆâŒ ä¸åœ¨æœ¬é¡¹ç›®å®æ–½èŒƒå›´ï¼‰
6. **A2C/A3C** â­â­â­ - âŒ ä¸åœ¨æœ¬é¡¹ç›®å®æ–½èŒƒå›´
7. **SAC** â­â­â­ - âŒ ä¸åœ¨æœ¬é¡¹ç›®å®æ–½èŒƒå›´

**é¡¹ç›®å®é™… baselines**:
- 1 RL baseline: DQN
- 3 Rule-based baselines: Strongest RSRP, A4-based Strategy, D2-based Strategy

è©³ç´°æ–‡ç»å¼•ç”¨è¦‹ [BASELINE_ALGORITHMS.md](BASELINE_ALGORITHMS.md)

---

## ğŸ¯ ç®—æ³•åˆ†é¡èˆ‡é›£åº¦

| é¡åˆ¥ | ç®—æ³• | é›£åº¦ | æ™‚é–“ | Environment ä¿®æ”¹ | Agent ä¿®æ”¹ |
|------|------|------|------|-----------------|-----------|
| **Value-based** | Double DQN, Dueling DQN | â­ Easy | 1-2å¤© | âŒ ä¸éœ€è¦ | âœ… å°æ”¹ |
| **Policy-based** | PPO, A2C | â­â­ Medium | 3-5å¤© | âŒ ä¸éœ€è¦ | âœ… é‡å¯« |
| **Continuous** | SAC, TD3 | â­â­â­ Medium-Hard | 5-7å¤© | âœ… éœ€è¦æ”¹ | âœ… é‡å¯« |
| **Model-based** | Dyna-Q, MBPO | â­â­â­â­ Hard | 7-14å¤© | âœ… éœ€è¦æ”¹ | âœ… é‡å¯« |
| **Multi-agent** | MADDPG, QMIX | â­â­â­â­â­ Very Hard | 14-21å¤© | âœ… å¤§æ”¹ | âœ… é‡å¯« |

---

## âœ… å®¹æ˜“æ›¿æ›ï¼šValue-Based Algorithms

### 1. Double DQN

**ä¿®æ”¹å…§å®¹**: åªæ”¹ Agent çš„ train_step()

**åŸç†**:
- DQN: ç”¨åŒä¸€å€‹ç¶²çµ¡é¸æ“‡å’Œè©•ä¼°å‹•ä½œ â†’ å®¹æ˜“é«˜ä¼° Q å€¼
- Double DQN: ç”¨ online network é¸æ“‡ï¼Œtarget network è©•ä¼°

**ä»£ç¢¼ä¿®æ”¹** (`src/agents/double_dqn_agent.py`):

```python
# DQN çš„æ›´æ–°
q_targets = rewards + gamma * target_q_values.max(1)[0]

# Double DQN çš„æ›´æ–°
# 1. ç”¨ online network é¸æ“‡æœ€ä½³å‹•ä½œ
best_actions = self.q_network(next_states).argmax(1)

# 2. ç”¨ target network è©•ä¼°è©²å‹•ä½œçš„ Q å€¼
q_targets = rewards + gamma * target_q_network(next_states).gather(1, best_actions)
```

**é æœŸæ”¹é€²**: æ¸›å°‘ Q å€¼éä¼°è¨ˆï¼Œæ›´ç©©å®šçš„è¨“ç·´

**åƒè€ƒè«–æ–‡**: Deep Reinforcement Learning with Double Q-learning (AAAI 2016)

---

### 2. Dueling DQN

**ä¿®æ”¹å…§å®¹**: æ”¹ QNetwork æ¶æ§‹

**åŸç†**: åˆ†é›¢ Value å’Œ Advantage
- V(s): ç‹€æ…‹åƒ¹å€¼ï¼ˆé€™å€‹ç‹€æ…‹æœ¬èº«æœ‰å¤šå¥½ï¼‰
- A(s,a): å‹•ä½œå„ªå‹¢ï¼ˆé€™å€‹å‹•ä½œæ¯”å…¶ä»–å‹•ä½œå¥½å¤šå°‘ï¼‰
- Q(s,a) = V(s) + A(s,a)

**ä»£ç¢¼ä¿®æ”¹** (`src/agents/dueling_dqn_network.py`):

```python
class DuelingQNetwork(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()

        # Shared feature extractor
        self.feature = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
        )

        # Value stream: V(s)
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)  # Single value
        )

        # Advantage stream: A(s,a)
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)  # Per-action advantages
        )

    def forward(self, x):
        features = self.feature(x)
        value = self.value_stream(features)
        advantages = self.advantage_stream(features)

        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))
        return q_values
```

**é æœŸæ”¹é€²**: æ›´å¥½åœ°å­¸ç¿’ç‹€æ…‹åƒ¹å€¼ï¼Œç‰¹åˆ¥é©åˆæŸäº›å‹•ä½œå½±éŸ¿è¼ƒå°çš„å ´æ™¯

**åƒè€ƒè«–æ–‡**: Dueling Network Architectures for Deep RL (ICML 2016)

---

## âš ï¸ ä¸­ç­‰é›£åº¦ï¼šPolicy-Based Algorithms

### PPO (Proximal Policy Optimization)

**ç‚ºä»€éº¼é¸æ“‡ PPOï¼Ÿ**
- âœ… ç•¶å‰æœ€æµè¡Œçš„ policy gradient æ–¹æ³•
- âœ… ç©©å®šã€æ¨£æœ¬æ•ˆç‡é«˜
- âœ… é©åˆè¡›æ˜Ÿæ›æ‰‹ï¼ˆé€£çºŒæ±ºç­–ï¼‰

**ä¿®æ”¹å…§å®¹**:
1. æ–°å¢ Policy Networkï¼ˆè¼¸å‡ºå‹•ä½œæ¦‚ç‡ï¼‰
2. æ–°å¢ Value Networkï¼ˆä¼°è¨ˆç‹€æ…‹åƒ¹å€¼ï¼‰
3. GAE (Generalized Advantage Estimation)
4. Clipped surrogate objective

**æ¶æ§‹å°æ¯”**:

```
DQN:
  Input â†’ Q-Network â†’ Q-values â†’ argmax â†’ Action

PPO:
  Input â†’ Policy Network â†’ Action Probabilities â†’ Sample â†’ Action
  Input â†’ Value Network â†’ V(s) â†’ ç”¨æ–¼è¨ˆç®— Advantage
```

**æ ¸å¿ƒä»£ç¢¼** (`src/agents/ppo_agent.py`):

```python
class PPOAgent:
    def __init__(self, obs_space, action_space, config):
        self.policy = PolicyNetwork(...)  # è¼¸å‡ºå‹•ä½œæ¦‚ç‡
        self.value = ValueNetwork(...)    # ä¼°è¨ˆç‹€æ…‹åƒ¹å€¼

        self.clip_epsilon = 0.2  # PPO clipping parameter
        self.gae_lambda = 0.95   # GAE lambda

    def select_action(self, state):
        # å¾ policy è¼¸å‡ºçš„æ¦‚ç‡åˆ†ä½ˆä¸­æ¡æ¨£
        probs = self.policy(state)
        dist = torch.distributions.Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action, log_prob

    def train_step(self, trajectories):
        # 1. è¨ˆç®— GAE advantages
        advantages = self.compute_gae(trajectories)

        # 2. PPO clipped objective
        ratio = new_probs / old_probs
        clipped_ratio = torch.clamp(ratio, 1-self.clip_epsilon, 1+self.clip_epsilon)
        loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()

        # 3. Update policy and value networks
        ...
```

**Training Loop ä¿®æ”¹**:

```python
# DQN: æ¯æ­¥æ›´æ–°
for step in episode:
    action = agent.select_action(state)
    next_state, reward = env.step(action)
    agent.store_experience(...)
    agent.train_step()  # æ¯æ­¥éƒ½å¯ä»¥è¨“ç·´

# PPO: æ‰¹æ¬¡æ›´æ–°ï¼ˆéœ€è¦å®Œæ•´è»Œè·¡ï¼‰
trajectory = []
for step in episode:
    action, log_prob = agent.select_action(state)
    next_state, reward = env.step(action)
    trajectory.append((state, action, reward, log_prob))

# Episode çµæŸå¾Œæ‰è¨“ç·´
agent.train_step(trajectory)
```

**é ä¼°æ™‚é–“**: 3-5 å¤©

**åƒè€ƒè«–æ–‡**: Proximal Policy Optimization Algorithms (arXiv 2017)

---

## âš ï¸âš ï¸ ä¸­ç­‰åé›£ï¼šContinuous Action Space

### SAC (Soft Actor-Critic)

**ç‚ºä»€éº¼é›£ï¼Ÿ**
- âŒ è¡›æ˜Ÿæ›æ‰‹æœ¬è³ªæ˜¯é›¢æ•£å‹•ä½œï¼ˆé¸æ“‡å“ªé¡†è¡›æ˜Ÿï¼‰
- âš ï¸ éœ€è¦é‡æ–°å®šç¾©å‹•ä½œç©ºé–“

**å¯èƒ½çš„é€£çºŒå‹•ä½œå®šç¾©**:

**é¸é … 1**: é€£çºŒè¡›æ˜Ÿ ID
```python
# ç•¶å‰ (Discrete)
action_space = Discrete(11)  # 0-10: stay or switch to satellite 0-9

# é€£çºŒç‰ˆæœ¬
action_space = Box(low=0, high=100, shape=(1,))  # è¡›æ˜Ÿ ID (é€£çºŒ)
# å•é¡Œï¼šå¦‚ä½•å¾é€£çºŒå€¼æ˜ å°„åˆ°å¯¦éš›è¡›æ˜Ÿï¼Ÿ
```

**é¸é … 2**: å¤šç¶­é€£çºŒå‹•ä½œ
```python
action_space = Box(low=-1, high=1, shape=(3,))
# [0]: æ˜¯å¦æ›æ‰‹ (sigmoid â†’ 0/1)
# [1-2]: ç›®æ¨™è¡›æ˜Ÿç‰¹å¾µ (elevation, azimuth) â†’ æ‰¾æœ€æ¥è¿‘çš„è¡›æ˜Ÿ
```

**å•é¡Œ**ï¼š
- è¡›æ˜Ÿæ›æ‰‹å¤©ç„¶æ˜¯é›¢æ•£æ±ºç­–
- å¼·è¡Œæ”¹æˆé€£çºŒå¯èƒ½é™ä½æ€§èƒ½
- è«–æ–‡éƒ½ç”¨é›¢æ•£å‹•ä½œ

**å»ºè­°**: é™¤éæœ‰ç‰¹æ®Šéœ€æ±‚ï¼Œå¦å‰‡ä¿æŒ Discrete action space

**å¦‚æœçœŸçš„è¦å¯¦ç¾ SAC**:
- ä¿®æ”¹ Environment: action_space æ”¹ç‚º Box
- å¯¦ç¾ action â†’ satellite æ˜ å°„
- SAC Agent: Actor-Critic + Entropy regularization
- é ä¼°æ™‚é–“: 5-7 å¤©

---

## ğŸ“ å¯¦ç”¨å»ºè­°

### æ¨è–¦å˜—è©¦çš„ç®—æ³•ï¼ˆå„ªå…ˆé †åºï¼‰

1. **Double DQN** (1å¤©)
   - âœ… ç°¡å–®ã€æœ‰æ•ˆ
   - âœ… æ¸›å°‘ Q å€¼éä¼°è¨ˆ
   - âœ… èˆ‡è«–æ–‡å°æ¯”å…¬å¹³ï¼ˆéƒ½æ˜¯ DQN ç³»åˆ—ï¼‰

2. **Dueling DQN** (2å¤©)
   - âœ… æ¶æ§‹æ”¹é€²
   - âœ… å¯èƒ½æå‡æ€§èƒ½
   - âœ… é©åˆæ›æ‰‹å•é¡Œï¼ˆæŸäº›ç‹€æ…‹ä¸‹å‹•ä½œå·®ç•°å°ï¼‰

3. **PPO** (3-5å¤©)
   - âœ… ç•¶å‰æœ€æµè¡Œ
   - âœ… ç©©å®šæ€§å¥½
   - âœ… å¯ä»¥ä½œç‚ºå°æ¯”å¯¦é©—ï¼ˆDQN vs PPOï¼‰

4. **Rainbow DQN** (2-3å¤©)
   - âœ… çµ„åˆå¤šå€‹æ”¹é€²ï¼ˆDouble, Dueling, Prioritized Replay, etc.ï¼‰
   - âœ… SOTA performance
   - âš ï¸ è¤‡é›œåº¦é«˜

### ä¸å»ºè­°å˜—è©¦çš„ç®—æ³•

1. **Continuous algorithms (SAC, TD3)**
   - âŒ æ›æ‰‹æœ¬è³ªæ˜¯é›¢æ•£æ±ºç­–
   - âŒ å¼·è¡Œæ”¹æˆé€£çºŒæ²’æœ‰æ˜é¡¯å¥½è™•

2. **Model-based RL**
   - âŒ éœ€è¦å­¸ç¿’è»Œé“æ¨¡å‹ï¼ˆæˆ‘å€‘å·²ç¶“æœ‰ SGP4ï¼‰
   - âŒ è¨ˆç®—é‡å¤§

3. **Multi-agent RL**
   - âŒ å•é¡Œå®šç¾©ä¸æ¸…æ¥šï¼ˆèª°æ˜¯ agentï¼Ÿï¼‰
   - âŒ å¯¦ç¾è¤‡é›œåº¦å¤ªé«˜

---

## ğŸ”§ å¿«é€Ÿå¯¦ç¾æ­¥é©Ÿ

### å¯¦ç¾ Double DQN (1å¤©)

```bash
# 1. è¤‡è£½ç•¶å‰ Agent
cp src/agents/dqn_agent_v2.py src/agents/double_dqn_agent.py

# 2. ä¿®æ”¹ train_step() æ–¹æ³•
#    - æ”¹ç”¨ online network é¸æ“‡å‹•ä½œ
#    - ç”¨ target network è©•ä¼° Q å€¼

# 3. æ¸¬è©¦
python3 train_online_rl.py \
    --agent-type double_dqn \
    --num-episodes 100 \
    --output-dir output/double_dqn_test

# 4. å°æ¯”çµæœ
#    - DQN vs Double DQN reward curves
#    - TensorBoard å°æ¯”
```

### å¯¦ç¾ PPO (3-5å¤©)

```bash
# 1. å»ºç«‹æ–° Agent
touch src/agents/ppo_agent.py

# 2. å¯¦ç¾æ ¸å¿ƒçµ„ä»¶
#    - PolicyNetwork (è¼¸å‡ºå‹•ä½œæ¦‚ç‡)
#    - ValueNetwork (ä¼°è¨ˆç‹€æ…‹åƒ¹å€¼)
#    - GAE è¨ˆç®—
#    - PPO loss

# 3. ä¿®æ”¹ training loop
#    - æ”¶é›†å®Œæ•´ trajectory
#    - Batch update

# 4. æ¸¬è©¦
python3 train_online_rl.py \
    --agent-type ppo \
    --num-episodes 500 \
    --output-dir output/ppo_test
```

---

## ğŸ“š åƒè€ƒè³‡æº

### è«–æ–‡

1. **DQN**: Playing Atari with Deep Reinforcement Learning (DeepMind 2013)
2. **Double DQN**: Deep RL with Double Q-learning (AAAI 2016)
3. **Dueling DQN**: Dueling Network Architectures for Deep RL (ICML 2016)
4. **Rainbow**: Rainbow: Combining Improvements in Deep RL (AAAI 2018)
5. **PPO**: Proximal Policy Optimization Algorithms (arXiv 2017)

### å¯¦ç¾åƒè€ƒ

1. **Stable-Baselines3**: https://github.com/DLR-RM/stable-baselines3
   - é«˜è³ªé‡å¯¦ç¾ï¼ˆDQN, PPO, SAC, TD3 ç­‰ï¼‰
   - å¯ä»¥åƒè€ƒæ¥å£è¨­è¨ˆ

2. **CleanRL**: https://github.com/vwxyzjn/cleanrl
   - ç°¡æ½”ã€æ•™å­¸å‘çš„å¯¦ç¾
   - é©åˆå­¸ç¿’

3. **RLlib**: https://docs.ray.io/en/latest/rllib/
   - Ray çš„ RL åº«
   - æ”¯æŒåˆ†ä½ˆå¼è¨“ç·´

---

## âœ… ç¸½çµ

**ç•¶å‰æ¶æ§‹çš„å„ªå‹¢**:
- âœ… Environment å®Œå…¨ç¬¦åˆ Gymnasium æ¨™æº–
- âœ… å¯ä»¥è¼•é¬†æ›¿æ› Value-based algorithms (1-2å¤©) - å¦‚æœå°†æ¥éœ€è¦
- âœ… å¯ä»¥å¯¦ç¾ Policy-based algorithms (3-5å¤©) - å¦‚æœå°†æ¥éœ€è¦
- âš ï¸ Continuous algorithms éœ€è¦ä¿®æ”¹ Environment (5-7å¤©) - å¦‚æœå°†æ¥éœ€è¦

**æœ¬é¡¹ç›®å®é™…å®æ–½** (Phase 1-2):
1. âœ… DQN baseline (å·²å®Œæˆ)
2. âœ… 3 Rule-based baselines: Strongest RSRP, A4-based Strategy, D2-based Strategy
3. âŒ ä¸å®æ–½å…¶ä»– RL ç®—æ³•ï¼ˆD3QNã€PPOã€A2C ç­‰ï¼‰

**å®é™…è®ºæ–‡å®éªŒ**:
- RL baseline: DQN
- Rule-based baselines: Strongest RSRP, A4-based Strategy, D2-based Strategy
- æœªæ¥å¯¹æ¯”: ç”¨æˆ·è‡ªå·±çš„ç®—æ³• vs ä»¥ä¸Š 4 ä¸ª baselines

**æœ¬æ–‡æ¡£ä»·å€¼**: è¯´æ˜ Gymnasium æ¶æ„çš„å¯æ‰©å±•æ€§ï¼Œä»¥åŠå¦‚ä½•å®ç°å…¶ä»–ç®—æ³•ï¼ˆå¦‚æœå°†æ¥éœ€è¦ï¼‰

---

**Date**: 2025-10-20
**Status**: âœ… æ¶æ§‹è¨­è¨ˆåƒè€ƒï¼ˆå¯¦éš›é …ç›®åªå¯¦æ–½ DQN + 3 Rule-based baselinesï¼‰
