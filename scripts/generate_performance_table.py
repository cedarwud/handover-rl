#!/usr/bin/env python3
"""
æ€§èƒ½å°æ¯”è¡¨æ ¼ç”Ÿæˆå™¨
ç”Ÿæˆè«–æ–‡ç´šçš„æ€§èƒ½å°æ¯”è¡¨æ ¼ï¼ˆLaTeX / Markdownï¼‰

Usage:
    # ç”Ÿæˆ LaTeX è¡¨æ ¼
    python scripts/generate_performance_table.py \\
        --data ours.log baseline1.log baseline2.log \\
        --labels "Ours" "Baseline 1" "Baseline 2" \\
        --output tables/performance_comparison.tex

    # ç”Ÿæˆ Markdown è¡¨æ ¼ï¼ˆç”¨æ–¼ READMEï¼‰
    python scripts/generate_performance_table.py \\
        --data ours.log baseline1.log \\
        --labels "Ours" "Baseline" \\
        --format markdown \\
        --output tables/performance_comparison.md
"""

import argparse
import sys
from pathlib import Path
import pandas as pd
import numpy as np

# å°å…¥æ•¸æ“šæå–å·¥å…·
sys.path.insert(0, str(Path(__file__).parent.parent))
from scripts.extract_training_data import extract_episode_data, compute_statistics


def generate_latex_table(results: pd.DataFrame,
                         output_file: str = None,
                         caption: str = None,
                         label: str = 'tab:performance') -> str:
    """
    ç”Ÿæˆ LaTeX è¡¨æ ¼

    Args:
        results: æ€§èƒ½çµæœ DataFrame
        output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        caption: è¡¨æ ¼æ¨™é¡Œ
        label: LaTeX æ¨™ç±¤

    Returns:
        LaTeX è¡¨æ ¼å­—ç¬¦ä¸²
    """

    if caption is None:
        caption = "Performance comparison of different methods on LEO satellite handover task."

    # LaTeX è¡¨æ ¼æ¨¡æ¿
    latex = []
    latex.append(r"\begin{table}[t]")
    latex.append(r"    \centering")
    latex.append(r"    \caption{" + caption + r"}")
    latex.append(r"    \label{" + label + r"}")

    # è¡¨æ ¼åˆ—æ•¸
    n_cols = len(results.columns)

    # è¡¨æ ¼å°é½Šï¼ˆç¬¬ä¸€åˆ—å·¦å°é½Šï¼Œå…¶ä»–å±…ä¸­ï¼‰
    col_align = "l" + "c" * (n_cols - 1)

    latex.append(r"    \begin{tabular}{" + col_align + r"}")
    latex.append(r"        \toprule")

    # è¡¨é ­
    headers = " & ".join(results.columns) + r" \\"
    latex.append(r"        " + headers)
    latex.append(r"        \midrule")

    # æ•¸æ“šè¡Œ
    for _, row in results.iterrows():
        # æ ¼å¼åŒ–æ•¸å€¼
        formatted_row = []
        for i, (col, val) in enumerate(zip(results.columns, row)):
            if i == 0:  # æ–¹æ³•åç¨±
                formatted_row.append(str(val))
            elif isinstance(val, str):
                formatted_row.append(val)
            else:
                # æ•¸å€¼ï¼šä¿ç•™ 2 ä½å°æ•¸
                formatted_row.append(f"{val:.2f}" if not np.isnan(val) else "-")

        row_str = " & ".join(formatted_row) + r" \\"
        latex.append(r"        " + row_str)

    latex.append(r"        \bottomrule")
    latex.append(r"    \end{tabular}")
    latex.append(r"\end{table}")

    latex_str = "\n".join(latex)

    # å„²å­˜åˆ°æª”æ¡ˆ
    if output_file:
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w') as f:
            f.write(latex_str)

        print(f"ğŸ’¾ LaTeX è¡¨æ ¼å·²å„²å­˜: {output_path}")

    return latex_str


def generate_markdown_table(results: pd.DataFrame,
                           output_file: str = None) -> str:
    """
    ç”Ÿæˆ Markdown è¡¨æ ¼

    Args:
        results: æ€§èƒ½çµæœ DataFrame
        output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘

    Returns:
        Markdown è¡¨æ ¼å­—ç¬¦ä¸²
    """

    # ä½¿ç”¨ pandas çš„ to_markdown() æ–¹æ³•
    markdown = results.to_markdown(index=False, floatfmt=".2f")

    # å„²å­˜åˆ°æª”æ¡ˆ
    if output_file:
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w') as f:
            f.write(markdown)

        print(f"ğŸ’¾ Markdown è¡¨æ ¼å·²å„²å­˜: {output_path}")

    return markdown


def create_performance_comparison(data_files: list,
                                  labels: list,
                                  include_steps: bool = True) -> pd.DataFrame:
    """
    å‰µå»ºæ€§èƒ½å°æ¯”è¡¨æ ¼

    Args:
        data_files: è¨“ç·´æ—¥èªŒæª”æ¡ˆåˆ—è¡¨
        labels: æ–¹æ³•æ¨™ç±¤åˆ—è¡¨
        include_steps: æ˜¯å¦åŒ…å«è¨“ç·´æ­¥æ•¸

    Returns:
        æ€§èƒ½å°æ¯” DataFrame
    """

    results = {
        'Method': [],
        'Final Reward': [],
        'Best Reward': [],
        'Avg Handovers': [],
        'Final Loss': [],
    }

    if include_steps:
        results['Training Episodes'] = []

    for log_file, label in zip(data_files, labels):
        print(f"ğŸ“Š åˆ†æ: {label} ({log_file})")

        # æå–æ•¸æ“š
        data = extract_episode_data(Path(log_file))

        if len(data) == 0:
            print(f"âš ï¸  è­¦å‘Š: {log_file} ç„¡æœ‰æ•ˆæ•¸æ“š")
            continue

        # è¨ˆç®—çµ±è¨ˆé‡
        stats = compute_statistics(data)

        # æ·»åŠ åˆ°çµæœ
        results['Method'].append(label)

        # Final Reward (mean Â± std)
        if stats['final_reward_mean'] is not None:
            results['Final Reward'].append(
                f"{stats['final_reward_mean']:.2f}Â±{stats['final_reward_std']:.2f}"
            )
        else:
            results['Final Reward'].append("-")

        # Best Reward
        if stats['best_reward'] is not None:
            results['Best Reward'].append(stats['best_reward'])
        else:
            results['Best Reward'].append(np.nan)

        # Avg Handovers
        if stats['avg_handovers'] is not None:
            results['Avg Handovers'].append(stats['avg_handovers'])
        else:
            results['Avg Handovers'].append(np.nan)

        # Final Loss
        if stats['final_loss'] is not None:
            results['Final Loss'].append(stats['final_loss'])
        else:
            results['Final Loss'].append(np.nan)

        # Training Episodes
        if include_steps:
            results['Training Episodes'].append(stats['total_episodes'])

    return pd.DataFrame(results)


def create_ablation_study_table(data_files: list,
                                labels: list,
                                baseline_idx: int = 0) -> pd.DataFrame:
    """
    å‰µå»º Ablation Study è¡¨æ ¼ï¼ˆé¡¯ç¤ºç›¸å°æ”¹é€²ï¼‰

    Args:
        data_files: è¨“ç·´æ—¥èªŒæª”æ¡ˆåˆ—è¡¨
        labels: æ–¹æ³•æ¨™ç±¤åˆ—è¡¨
        baseline_idx: Baseline æ–¹æ³•çš„ç´¢å¼•

    Returns:
        Ablation Study DataFrame
    """

    # å…ˆå‰µå»ºåŸºæœ¬çš„æ€§èƒ½å°æ¯”è¡¨æ ¼
    basic_results = create_performance_comparison(data_files, labels, include_steps=False)

    # æå– Baseline çš„æ€§èƒ½
    baseline_reward_str = basic_results.iloc[baseline_idx]['Final Reward']
    baseline_reward = float(baseline_reward_str.split('Â±')[0])

    # æ·»åŠ ç›¸å°æ”¹é€²åˆ—
    improvements = []
    for _, row in basic_results.iterrows():
        reward_str = row['Final Reward']
        if reward_str == "-":
            improvements.append("-")
        else:
            reward = float(reward_str.split('Â±')[0])
            improvement = ((reward - baseline_reward) / abs(baseline_reward)) * 100
            improvements.append(f"{improvement:+.1f}%")

    basic_results['Improvement'] = improvements

    return basic_results


def main():
    parser = argparse.ArgumentParser(
        description='ç”Ÿæˆæ€§èƒ½å°æ¯”è¡¨æ ¼ï¼ˆLaTeX / Markdownï¼‰'
    )
    parser.add_argument('--data', nargs='+', required=True,
                       help='è¨“ç·´æ—¥èªŒæª”æ¡ˆè·¯å¾‘ï¼ˆå¯å¤šå€‹ï¼‰')
    parser.add_argument('--labels', nargs='+', required=True,
                       help='æ–¹æ³•æ¨™ç±¤ï¼ˆèˆ‡ --data å°æ‡‰ï¼‰')
    parser.add_argument('--output', '-o', type=str, required=True,
                       help='è¼¸å‡ºæª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--format', choices=['latex', 'markdown'], default='latex',
                       help='è¼¸å‡ºæ ¼å¼ï¼ˆé è¨­ latexï¼‰')
    parser.add_argument('--caption', type=str, default=None,
                       help='è¡¨æ ¼æ¨™é¡Œï¼ˆåƒ… LaTeXï¼‰')
    parser.add_argument('--label', type=str, default='tab:performance',
                       help='LaTeX æ¨™ç±¤ï¼ˆé è¨­ tab:performanceï¼‰')
    parser.add_argument('--ablation', action='store_true',
                       help='ç”Ÿæˆ Ablation Study è¡¨æ ¼ï¼ˆé¡¯ç¤ºç›¸å°æ”¹é€²ï¼‰')
    parser.add_argument('--baseline-idx', type=int, default=0,
                       help='Baseline æ–¹æ³•çš„ç´¢å¼•ï¼ˆé è¨­ 0ï¼‰')

    args = parser.parse_args()

    # æª¢æŸ¥æ•¸æ“šå’Œæ¨™ç±¤æ•¸é‡æ˜¯å¦åŒ¹é…
    if len(args.data) != len(args.labels):
        print(f"âŒ éŒ¯èª¤: æ•¸æ“šæª”æ¡ˆæ•¸é‡ ({len(args.data)}) èˆ‡æ¨™ç±¤æ•¸é‡ ({len(args.labels)}) ä¸ç¬¦")
        return 1

    print("="*70)
    print("æ€§èƒ½å°æ¯”è¡¨æ ¼ç”Ÿæˆå™¨")
    print("="*70)

    # å‰µå»ºæ€§èƒ½å°æ¯”è¡¨æ ¼
    if args.ablation:
        print("\nğŸ“Š ç”Ÿæˆ Ablation Study è¡¨æ ¼...")
        results = create_ablation_study_table(args.data, args.labels, args.baseline_idx)
    else:
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½å°æ¯”è¡¨æ ¼...")
        results = create_performance_comparison(args.data, args.labels)

    # é¡¯ç¤ºè¡¨æ ¼é è¦½
    print("\n" + "="*70)
    print("ğŸ“‹ è¡¨æ ¼é è¦½:")
    print("="*70)
    print(results.to_string(index=False))
    print("="*70)

    # ç”Ÿæˆè¼¸å‡º
    if args.format == 'latex':
        latex_str = generate_latex_table(results, args.output, args.caption, args.label)
        print("\n" + "="*70)
        print("ğŸ“„ LaTeX ç¨‹å¼ç¢¼:")
        print("="*70)
        print(latex_str)
        print("="*70)
        print("\nğŸ’¡ ä½¿ç”¨å»ºè­°:")
        print("   1. è¤‡è£½ä¸Šè¿° LaTeX ç¨‹å¼ç¢¼åˆ°è«–æ–‡ä¸­")
        print("   2. ç¢ºä¿ preamble ä¸­æœ‰: \\usepackage{booktabs}")
        print("   3. è¡¨æ ¼æœƒè‡ªå‹•ç½®é ‚ (table[t])")

    else:  # markdown
        md_str = generate_markdown_table(results, args.output)
        print("\n" + "="*70)
        print("ğŸ“„ Markdown ç¨‹å¼ç¢¼:")
        print("="*70)
        print(md_str)
        print("="*70)

    print("\nâœ… è¡¨æ ¼ç”Ÿæˆå®Œæˆï¼")

    return 0


if __name__ == '__main__':
    exit(main())
