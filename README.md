# Handover-RL: LEO Satellite Handover Optimization with RL

**Modular reinforcement learning framework for optimizing LEO satellite handover**

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-1.0+-green.svg)](https://gymnasium.farama.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ðŸŽ¯ Current Status

- âœ… **BC Training Complete**: 88.81% accuracy (target: 85-95%) - See [BC V4 Report](docs/reports/TRAINING_REPORT_V4_FINAL.md)
- âœ… **Data Leakage Fixed**: Eliminated 100% accuracy problem - See [Diagnosis](docs/reports/DIAGNOSIS_100_ACCURACY.md)
- âœ… **Threshold Design**: Data-driven (-34.5 dBm) - See [Recommendations](docs/reports/FINAL_THRESHOLD_RECOMMENDATIONS.md)
- ðŸ“ **Next**: DQN Training with BC warm-start - See [Project Status](docs/PROJECT_STATUS.md)
- âœ… **Gymnasium Environment**: Standards-compliant, algorithm-agnostic
- âœ… **Multi-Level Training**: 10 minutes â†’ 35 hours progressive strategy

**Last Updated**: 2025-10-21

---

## ðŸš€ Quick Start

### Prerequisites

**Software**:
- Python 3.10+
- PyTorch 2.0+
- orbit-engine installed at `../orbit-engine`

**Hardware**:
- **RAM**: 8GB+ (16GB recommended for Level 3+)
- **CPU**: Multi-core processor (4+ cores recommended)
- **GPU**: Optional but recommended for faster training
  - CUDA-capable GPU with 4GB+ VRAM (e.g., NVIDIA GTX 1650 or better)
  - CPU-only training supported but slower
- **Storage**: 2GB+ free space for data and models

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/handover-rl.git
cd handover-rl

# Setup environment
./setup_env.sh all

# Activate virtual environment
source venv/bin/activate
```

### Run Training

```bash
# Level 1: Quick validation (2 hours, 100 episodes)
./quick_train.sh 1

# Level 3: Validation (10 hours, 500 episodes) - Recommended
./quick_train.sh 3

# Level 5: Full training (35 hours, 1700 episodes) - Publication quality
./quick_train.sh 5
```

**See [Quick Start Guide](docs/training/QUICKSTART.md) for details**

---

## ðŸ“ Project Structure

```
handover-rl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environments/              # Gymnasium environment
â”‚   â”‚   â””â”€â”€ satellite_handover_env.py  âœ… Algorithm-agnostic
â”‚   â”œâ”€â”€ agents/                    # RL algorithms
â”‚   â”‚   â”œâ”€â”€ dqn_agent_v2.py       âœ… DQN (current)
â”‚   â”‚   â””â”€â”€ ...                    ðŸš§ PPO, Double DQN, etc. (Phase 1-3)
â”‚   â”œâ”€â”€ strategies/                # Rule-based comparison methods
â”‚   â”‚   â””â”€â”€ ...                    ðŸš§ A4, D2, heuristics (Phase 4)
â”‚   â”œâ”€â”€ trainers/                  ðŸš§ Training logic (refactoring)
â”‚   â””â”€â”€ utils/                     # Utilities
â”‚       â””â”€â”€ satellite_utils.py    âœ… Stage 4 pool loading
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ architecture/              # Architecture design
â”‚   â”œâ”€â”€ training/                  # Training guides
â”‚   â”œâ”€â”€ algorithms/                # Algorithm baselines & literature
â”‚   â””â”€â”€ development/               # Implementation plans
â”œâ”€â”€ config/                        # Configuration files
â”‚   â”œâ”€â”€ data_gen_config.yaml
â”‚   â””â”€â”€ training_config.yaml
â”œâ”€â”€ train_online_rl.py            âœ… Current training script
â””â”€â”€ README.md                      # This file
```

---

## ðŸ“Š System Architecture

### Current Implementation

```
Environment (Gymnasium) âœ…
    â†“
DQN Agent âœ…
    â†“
Online Training with Experience Replay âœ…
    â†“
Multi-Level Training Strategy âœ…
```

### Planned Refactoring

```
Unified Training Entry (train.py) ðŸš§
    â†“
Trainer Layer (Off-policy / On-policy) ðŸš§
    â†“
Agent Layer (DQN / PPO / A2C) ðŸš§
    â†“
Environment Layer (Gymnasium) âœ… Already done
```

**See [Architecture Refactor](docs/architecture/ARCHITECTURE_REFACTOR.md) for details**

---

## ðŸŽ“ Scientific Rigor

### Data Sources
- âœ… **Real TLE Data**: Space-Track.org (79 Starlink TLE files, 82 days coverage)
- âœ… **Official Physics Models**:
  - ITU-R P.676-13 (atmospheric attenuation)
  - 3GPP TS 38.214 (RSRP/RSRQ/SINR calculations)
  - 3GPP TS 38.331 (A3/A4/A5/D2 handover events)
- âœ… **No Simplified Algorithms**: All implementations follow official specifications
- âœ… **No Mock Data**: Only real physical calculations from orbit-engine

### Constellation Choice
- **Starlink-only** (101 satellites)
- **Rationale**: Cross-constellation handover (Starlinkâ†”OneWeb) not realistic
  - Literature review: NO papers do cross-constellation handover
  - Commercial reality: Separate networks (like AT&T vs Verizon)

**See [Constellation Choice](docs/architecture/CONSTELLATION_CHOICE.md)**

---

## ðŸ§ª Multi-Level Training Strategy

### Progressive Validation

| Level | Satellites | Episodes | Duration | Use Case |
|-------|-----------|----------|----------|----------|
| 0 | 10 | 10 | 10 min | Smoke test |
| 1 | 20 | 100 | 2 hours | Quick validation â­ Start here |
| 2 | 50 | 300 | 6 hours | Development |
| 3 | 101 | 500 | 10 hours | Validation (paper draft) |
| 4 | 101 | 1000 | 21 hours | Baseline (paper experiments) |
| 5 | 101 | 1700 | 35 hours | Full training (final experiments) |

**Rationale**: Avoid 35-hour training every iteration. Progressive validation enables faster development.

**See [Training Levels](docs/training/TRAINING_LEVELS.md)**

---

## ðŸ“š Algorithm Support

### Project Scope (Baseline Framework)

**RL Baseline** (Phase 1):
- âœ… **DQN** (Deep Q-Network) - Standard RL baseline from literature

**Rule-based Baselines** (Phase 2):
- âœ… **Strongest RSRP** - Simple heuristic strategy
- âœ… **A4-based Strategy** - 3GPP A4 event + RSRP selection (validated for LEO)
- âœ… **D2-based Strategy** - 3GPP D2 event + distance selection (NTN-specific) â­

**Future Comparison**:
- â­ User's own algorithm vs above 4 baselines
- âŒ No need to implement other RL algorithms (D3QN, A2C, etc.)

### Literature Review Reference (Not in Scope)

For understanding LEO satellite handover research landscape, see [Baseline Algorithms](docs/algorithms/BASELINE_ALGORITHMS.md) which includes literature review of:
- D3QN, A2C, Rainbow DQN, SAC (æœ‰ handover è«–æ–‡è­‰æ“š)
- PPO (ç”¨æ–¼ satellite schedulingï¼Œéž handover)

**Note**: These are for reference only, not in project implementation scope

---

## ðŸ“– Documentation

**å®Œæ•´æ–‡æª”ç´¢å¼•**: [docs/README.md](docs/README.md) â­

### Current Status & Reports (2025-10-21) â­ NEW

| æ–‡æª” | èªªæ˜Ž |
|------|------|
| **[Project Status](docs/PROJECT_STATUS.md)** | ç•¶å‰é …ç›®ç‹€æ…‹èˆ‡å¾…è¾¦äº‹é … â­ |
| **[BC è¨“ç·´ç¸½çµ](docs/reports/FINAL_SOLUTION_SUMMARY.md)** | å®Œæ•´è§£æ±ºæ–¹æ¡ˆï¼ˆå¿…è®€ï¼‰|
| **[è¨“ç·´å ±å‘Š V4](docs/reports/TRAINING_REPORT_V4_FINAL.md)** | BC V4 è¨“ç·´è©³ç´°å ±å‘Š |
| **[æ•¸æ“šæ´©æ¼è¨ºæ–·](docs/reports/DIAGNOSIS_100_ACCURACY.md)** | 100% æº–ç¢ºçŽ‡å•é¡Œåˆ†æž |
| **[é–¾å€¼å»ºè­°](docs/reports/FINAL_THRESHOLD_RECOMMENDATIONS.md)** | æ•¸æ“šé©…å‹•é–¾å€¼è¨­è¨ˆ |
| **[æ¸…ç†å ±å‘Š](docs/reports/CLEANUP_REPORT.md)** | é …ç›®çµæ§‹æ•´ç†è¨˜éŒ„ |

### Quick References
- **[Quick Start](docs/training/QUICKSTART.md)** - Get started in 5 minutes
- **[Training Levels](docs/training/TRAINING_LEVELS.md)** - Multi-level strategy explained
- **[Gymnasium Migration](docs/training/GYMNASIUM_MIGRATION.md)** - Why Gymnasium, not gym

### Architecture & Design
- **[Architecture Refactor](docs/architecture/ARCHITECTURE_REFACTOR.md)** - Modular framework design
- **[Constellation Choice](docs/architecture/CONSTELLATION_CHOICE.md)** - Why Starlink-only
- **[Data Dependencies](docs/architecture/DATA_DEPENDENCIES.md)** - orbit-engine integration

### Algorithms & Research
- **[Baseline Algorithms](docs/algorithms/BASELINE_ALGORITHMS.md)** - Literature-backed algorithm selection
- **[Algorithm Guide](docs/algorithms/ALGORITHM_GUIDE.md)** - How to implement new algorithms
- **[Literature Review](docs/algorithms/LITERATURE_REVIEW.md)** - 2023-2025 papers summary

### Development
- **[Implementation Plan](docs/development/IMPLEMENTATION_PLAN.md)** - Phase-by-phase refactoring plan (ä¿®æ­£ç‰ˆ)
- **[Phase 2: Rule-based Methods](docs/development/PHASE2_RULE_BASED_METHODS.md)** - Detailed guide for implementing comparison methods â­

---

## ðŸ”¬ Research Contributions

### Novel Aspects
1. **Multi-Level Training Strategy**: 6 levels from 10min to 35hrs (progressive validation)
2. **Continuous Time Sampling**: Sliding window with configurable overlap
3. **Starlink-Specific**: 101 satellites from orbit-engine Stage 4 (real TLE data)
4. **Comprehensive Baseline Framework**: Modular architecture supporting RL and rule-based methods
5. **NTN-Specific Baselines**: First use of D2 event (3GPP Rel-17 NTN) as baseline â­

### Baseline Methods (For Algorithm Comparison)
**RL Baseline** (Phase 1):
- DQN (Deep Q-Network) - Standard RL baseline from literature

**Rule-based Baselines** (Phase 2):
- Strongest RSRP (Simple heuristic)
- A4-based Strategy (3GPP event + RSRP selection, validated for LEO)
- D2-based Strategy (3GPP Rel-17 event + distance selection, NTN-specific) â­

**Note**: A4/D2 are 3GPP measurement report triggers. Our baseline strategies supplement them with selection logic and handover decisions.

### Academic Compliance
- âœ… All parameters traceable to official sources
- âœ… No hardcoded values (satellite pool from Stage 4)
- âœ… No simplified algorithms
- âœ… Reproducible (seed-controlled)
- âœ… Rule-based methods use 3GPP standards + orbit-engine real data

---

## ðŸ“Š Performance Baselines

Based on literature (Graph RL, Frontiers 2023):
- **Handover Frequency**: 10-30% of timesteps (target)
- **Average RSRP**: > -95 dBm
- **Convergence**: ~1500-1700 episodes
- **Ping-Pong Rate**: < 10%

---

## ðŸ› ï¸ Development Roadmap

### Phase 1: DQN Refactoring (Week 1-2)
- [ ] Create BaseAgent interface
- [ ] Implement OffPolicyTrainer
- [ ] Refactor DQN to new architecture
- [ ] **Preserve Multi-Level Training (P0 Critical)** â­
- [ ] Validation against current implementation

### Phase 2: Rule-based Baselines (Week 3)
- [ ] Implement 3 rule-based strategies (Strongest RSRP, A4-based, D2-based)
- [ ] Unified evaluation framework (RL + rule-based)
- [ ] Level 1 baseline evaluation (DQN + 3 rule-based)
- [ ] Baseline framework complete and ready for algorithm comparison

**Total Time**: 2-3 weeks

**ç›®æ¨™**: å»ºç«‹å®Œæ•´çš„ baseline æ¡†æž¶ï¼ŒåŒ…å« 1 å€‹ RL baseline (DQN) å’Œ 3 å€‹ rule-based baselinesï¼Œä½œç‚ºæœªä¾†ç®—æ³•å°æ¯”çš„åŸºç¤Ž
**See [Implementation Plan](docs/development/IMPLEMENTATION_PLAN.md)** | **[BASELINE_ALGORITHMS.md](docs/algorithms/BASELINE_ALGORITHMS.md)**

---

## ðŸ“„ Citation

If you use this code in your research, please cite:

```bibtex
@software{handover_rl_2025,
  title={Handover-RL: Modular RL Framework for LEO Satellite Handover},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/handover-rl}
}
```

---

## ðŸ¤ Contributing

Contributions welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## ðŸ“ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

---

## ðŸ”— Links

- **orbit-engine**: https://github.com/yourusername/orbit-engine
- **Gymnasium**: https://gymnasium.farama.org/
- **Literature**: See [docs/algorithms/BASELINE_ALGORITHMS.md](docs/algorithms/BASELINE_ALGORITHMS.md)

---

**Status**: ðŸš§ Active Development - Phase 1 (DQN Refactoring)
**Version**: 1.0.0-dev
**Last Updated**: 2025-10-20 (å»ºç«‹ Baseline æ¡†æž¶)
**Estimated Completion**: 2-3 weeks (Phase 1-2)
**ç›®æ¨™**: å»ºç«‹åŒ…å« DQN + Rule-based baselines çš„å®Œæ•´æ¡†æž¶ï¼Œä½œç‚ºæœªä¾†ç®—æ³•å°æ¯”çš„åŸºç¤Ž
